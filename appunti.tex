\documentclass[hidelinks, 10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{blindtext}
\usepackage{listings}
\usepackage{float}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{breqn}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{titling}
\usepackage{url}
\usepackage{array}
\usepackage{chngcntr}
\usepackage{tikz}
\usepackage[a4paper]{geometry}

\usetikzlibrary{arrows, automata, backgrounds, calendar, chains, matrix, mindmap, patterns, petri, shadows, shapes.geometric, shapes.misc, spy, trees}

\author{Andreas Veeser}
\date{A.A. 2017-2018}
\title{Calcolo numerico 2}

\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Mat}{Mat}

\begin{document}
\providecommand{\defeq}{\vcentcolon=}
\providecommand{\eqdef}{=\vcentcolon}
\providecommand{\compl}[1]{\prescript{c}{}{#1}}
\providecommand{\refsec}[1]{\S \ref{section:#1}}
\theoremstyle{plain}
\newtheorem{thm}{Teorema}[]

\theoremstyle{definition}
\newtheorem{defn}[]{Definizione}
\newtheorem{prop}[]{Proposizione}
\newtheorem{cor}[]{Corollario}
\newtheorem{lem}[]{Lemma}
\newtheorem{oss}[]{Osservazione}
\newtheorem{nota}[]{Nota}
\newtheorem{es}[]{Esempio}
\newtheorem{ex}[]{Esercizio}

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thepage}{\roman{page}}
\counterwithin*{equation}{section}
\renewcommand{\theequation}{\thesection.\Alph{equation}}
\renewcommand{\thefigure}{\thesection.\alph{figure}}
\maketitle

\tableofcontents

\chapter*{Avvertenze}
Queste dispense sono nate dal lavoro di alcuni studenti volenterosi che hanno trascritto in \LaTeX le lezioni del docente, cercando di offrire il miglior lavoro possibile da lasciare ai posteri. Dato che tale lavoro non \`e ancora stato revisionato dal docente, questo documento potrebbe contenere errori, presenti gi\`a sugli appunti del docente o generatisi durante la trascrizione.

Le persone che hanno contribuito alla realizzazione della dispensa non si assumono la responsabilit\`a di eventuali errori e invitano gli usufruitori, in attesa della revisione del docente, a inviare una email con eventuali errori, segnalazioni di figure mancanti o parziali o semplice un feedback per migliorare il materiale a michael.moroni@studenti.unimi.it

Data della compilazione: 3 maggio 2018.

\chapter{Introduzione}
\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}
\section{Discretizzazione di un problema ai valori iniziali}	\label{section:1}
\subsection{Problema ai valori iniziali}
Dati:
\begin{itemize}
\item $ t_{0} \in \mathbb{R} $ tempo iniziale;
\item $ T \in \mathbb{R} $ con $ T > t_{0} $ tempo finale;
\item $ I = [t_{0}, T] $ un intervallo;
\item $ f: I \times \mathbb{R} \to \mathbb{R} $ una funzione;
\item $ v \in \mathbb{R} $ valore iniziale;
\end{itemize}

l'obiettivo di un problema ai valori iniziali \`e quello di trovare $ u: I \to \mathbb{R} $ tale che
\[
\begin{cases}
u' = f(\cdot, u) \\
u(t_{0}) = v
\end{cases}
\]

o, meglio, $ \forall\ t \in I $

\begin{equation}	\label{eq:PVI}
\begin{cases}
u'(t) = f \left( t, u(t) \right) \\
u(t_{0}) = v
\end{cases}
\end{equation}

In generale $ u $ non \`e nota esplicitamente e quindi l'ottenere informazioni quantitative, come ad esempio $ u(T) $, richiede una risoluzione numerica. In $ (\ref{eq:PVI}) $ si celano un'infinit\`a di condizioni.

\subsection{Le griglie}
Un'equazione differenziale ordinaria (EDO) consiste di un numero infinito di condizioni, dato che $ t \in I $, linearmente indipendenti. Non si possono neanche verificare sul computer in tempo finito. Pertanto, si introduce una griglia (o \emph{mesh}) dell'intervallo $ I $:

\[ M_{N} = \{ t_{n} \defeq t_{0} + n \tau : n = 0, \dotsc, N \} \]

dove:
\begin{itemize}
\item $ N \in \mathbb{N} $ \`e il numero di passi;
\item $ \tau \defeq \frac{T - t_{0}}{N} $ \`e il passo temporale.
\end{itemize}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-4,0) -- (4,0) node[right] {$ t $};
	\filldraw (-3,0) circle (1pt) node[below] {$ t_{1} $};
	\filldraw (-2,0) circle (1pt) node[below] {$ t_{2} $};
	\filldraw (-1,0) circle (1pt) node[below] {$ t_{3} $};
	\fill (0,0) node[below] {$ \ldots $};
	\filldraw (1,0) circle (1pt) node[below] {$ t_{N - 2} $};;
	\filldraw (2,0) circle (1pt) node[below] {$ t_{N - 1} $};
	\filldraw (3,0) circle (1pt) node[below] {$ t_{N} $};
\end{tikzpicture}

\caption{Esempio di discretizzazione dell'asse delle $ t $. $ \tau $ \`e la distanza tra i vari nodi ed \`e costante.}
\end{center}
\end{figure}

\subsection{Sostituzione dell'operatore differenziale}
Data ad esempio $ t \in U_{N} $, neanche $ f \left( t, u(t) \right) = u'(t) $ \`e verificabile al computer poich\'e in generale

\[ u'(t) = \lim\limits_{h \to 0} \frac{u(t + h) - u(t)}{h} \]

coinvolge un numero infinito di operazioni. Se si usa l'approssimazione

\[ u'(t) \approx \frac{u(t + h) - u(t)}{h} \]

con $ t = t_{n} $ e $ h = t_{n + 1} - t_{n} = \tau $ si ottiene

\[
\begin{cases}
u(t_{0}) = v \\
\frac{u(t_{n + 1}) - u(t_{n})}{\tau} \approx f \left( t_{n}, u(t_{n}) \right)
\end{cases}
\]

Il pregio \`e che questo sistema \`e discreto, rispetto a $ (\ref{eq:PVI}) $.

Sostituendo $ \{ u(t_{n}) \}_{n = 0}^{N} $ con una successione finita incognita $ \{ U_{n} \} $ si ottiene

\begin{equation}	\label{eq:EE}
\begin{cases}
U_{0} = v \\
\frac{U_{n + 1} - U_{n}}{\tau} = f(t_{n}, U_{n})
\end{cases}
\end{equation}

Dal momento che $ \frac{U_{n + 1} - U_{n}}{\tau} = f(t_{n}, U_{n}) \iff U_{n + 1} = U_{n} + \tau f(t_{n}, U_{n}) $, gli $ U_{n} $ possono essere facilmente (persino velocemente) calcolati con un'interazione da $ U_{0} = v $. $ (\ref{eq:EE}) $ viene chiamato metodo di Eulero esplicito.

\section{Equazioni differenziali ordinarie}
\subsection{Richiamo sulle derivate}
Sia $ I $ un intervallo non degenere, cio\`e sia $ I $ in una delle seguenti forme:
\begin{itemize}
\item $ I = (a, b) $;
\item $ I = [a, b) $;
\item $ I = (a, b] $;
\item $ I = [a, b] $; 
\end{itemize}

con $ a, b \in \overline{\mathbb{R}} \defeq \mathbb{R} \cup \{ - \infty, + \infty \} $ tali che $ a < b $. Inoltre sia $ d \in \mathbb{N} $. Una funzione $ v: I \to \mathbb{R}^{d} $ si dice derivabile in $ t \in I $ se esiste

\[ v'(t) = \lim\limits_{s \to t} \frac{v(s) - v(t)}{s - t} \]

Dove definito, si pone, $ \forall\ t \in I $

\[
\begin{cases}
v^{(0)}(t) = v(t) \\
v^{(k + 1)}(t) = (v^{(k)})' (t) 
\end{cases}
\]

\subsection{Equazione differenziale ordinaria esplicita}
Siano $ d, k \in \mathbb{N} $, $ \Omega \subseteq \mathbb{R} \times \mathbb{R}^{kd} $ un insieme aperto e connesso.

\begin{itemize}
\item Una funzione $ u: I \to \mathbb{R}^{d} $ si dice soluzione dell'equazione differenziale ordinaria esplicita se

\[
u^{(k)} = f \left( \cdot, u^{(0)}, \dotsc, u^{(k - 1)} \right)
\]

cio\`e se $ \forall\ t \in I, \exists\ u^{(0)}(t), \dotsc, u^{(k)}(t) $ e 

\begin{equation}	\label{eq:EDO}
u^{(k)}(t) = f \left( t, u^{(0)}(t), \dotsc, u^{(k - 1)}(t) \right)
\end{equation}

$ d $ si dice dimensione dell'equazione differenziale ordinaria e $ k $ \`e il suo ordine.

\item $ (\ref{eq:EDO}) $ si dice autonomo se $ \exists\ \Omega_{0} \subseteq \mathbb{R}^{kd} $ e $ f_{0}: \Omega_{0} \to \mathbb{R}^{d} $ tale che $ \Omega = \mathbb{R} \times \Omega_{0} $ e $ \forall\ t \in \mathbb{R}, z \in \Omega_{0} $ si ha che $ f(t, z) = f_{0} (z) $;
\item $ (\ref{eq:EDO}) $ si dice lineare se 

\[ f(t, z) = \sum\limits_{i = 0}^{k - 1} a_{i} (t) z_{i} + b(t) \]

dove $ a_{0}, \dotsc, a_{k - 1}: \mathbb{R} \to \mathbb{R}^{d \times d} $ e $ b: \mathbb{R} \to \mathbb{R}^{d} $. Inoltre se $ b \equiv 0 $ si dice omogeneo, altrimenti non omogeneo.
\end{itemize}

\subsection{Qualche esempio}
\noindent
\begin{enumerate}
\item Dato $ \lambda \in \mathbb{R} $ si consideri $ u' = \lambda u $ in $ \mathbb{R} $. Tale equazione differenziale ha dimensione 1, ordine 1, \`e autonomo, lineare e omogeneo. Le soluzioni assumono la seguente forma:

\[ u(t) = C \exp(\lambda t) \]

con $ C \in \mathbb{R} $.

\begin{itemize}
\item Se $ \lambda > 0 $ si parla di crescita esponenziale ed \`e facilmente calcolabile con il metodo di Eulero esplicito;
\item Se $ \lambda < 0 $ si parla di decrescita esponenziale e si riscontrano problemi nel calcolo con il metodo di Eulero esplicito.
\end{itemize}

\item Un modello di crescita pi\`u flessibile \`e l'equazione di Verhulst

\[ u' = \lambda u \left( 1 - \frac{u}{k} \right) \]

che ha dimensione 1, ordine 1, \`e autonoma ma non \`e lineare. La soluzione generale \`e 

\[ u(t) = \frac{K C \exp (\lambda t)}{K + C (\exp (\lambda t) - 1)} \]

\item Dato un campo vettoriale $ f: \mathbb{R}^{3} \to \mathbb{R}^{3} $ la seconda legge di Newton pu\`o assumere la seguente forma in $ \mathbb{R} $:

\[ u'' = f(u) \]

che ha dimensione 3, ordine 2, \`e autonomo e in generale non \`e lineare. Il sottocaso lineare $ f(z) = - kz $, con $ z \in \mathbb{R}^{3} $, $ k \in \mathbb{R}^{d \times d} $ si dice oscillatore armonico;

\item Si consideri l'oscillatore armonico in una dimensione e con $ k  = 1 $:

\[ u'' = - u \]

Introducendo la nuova variabile $ v = u' $ si pu\`o riscrivere l'equazione con il seguente sistema

\[
\begin{cases}
u' = v \\
v' = -u
\end{cases} \iff \begin{pmatrix}
	u \\
	v \\
\end{pmatrix} ' = \begin{pmatrix}
	 0 & 1 \\
	-1 & 0 \\
\end{pmatrix} \begin{pmatrix}
	u \\
	v \\
\end{pmatrix}
\]

e con $ z = u + iv \in \mathbb{C} $ il problema diventa

\[ z' = -i z \]

poich\'e $ -i(u + iv) = -i u + v $. Questo corrisponde all'esempio $ 1. $ in $ \mathbb{C} $ con $ \lambda = -i $.
\end{enumerate}

\section{Problemi ai valori iniziali}
Le applicazioni e le soluzioni generali degli esempi nel paragrafo precedente suggeriscono di porre ulteriori condizioni per individuare una soluzione di un'equazione differenziale ordinaria.

\subsection{Problema ai valori iniziali}	\label{section:3.1}
Siano $ d, k \in \mathbb{N} $, $ \Omega \subseteq \mathbb{R} \times \mathbb{R}^{kd} $ dominio aperto e connesso. Sia $ f: \Omega \to \mathbb{R}^{d} $, $ (t_{0}, v_{0}, \dotsc, v_{k-1}) \in \Omega $, $ t_{0} < T $ e $ I = [t_{0}, T] $.

Una funzione $ u: I \to \mathbb{R}^{d} $ si dice soluzione di $ u^{(k)} = f(\cdot, u^{(0)}, \dotsc, u^{(k-1)}) $ con $ u^{(i)} (t_{0}) = v_{i} $ con $ i = 0, \dotsc, k - 1 $ se:
\begin{enumerate}
\item $ u $ \`e derivabile con continuit\`a $ k $ volte in $ I $ e $ \forall\ t \in I, \left( t, u^{(0)}(t), \dotsc, u^{(k-1)}(t) \right) \in \Omega $;
\item $ u^{(i)} (t_{0}) = v_{i} $ per $ i = 0, \dotsc, k - 1 $;
\item $ \forall\ t \in I, u^{(k)}(t) = f \left( t, u(t), \dotsc, u^{(k-1)}(t) \right) $.
\end{enumerate}

Introducendo nuove variabili, cio\`e aumentando $ d $, ci si pu\`o sempre  ridursi al caso $ k = 1 $. Da ora in poi si considerer\`a $ k = 1 $.

\subsection{Un esempio di non-unicit\`a}
Si consideri in $ [0, +\infty) $

\[ \begin{cases}
u' = 3 u^{\frac{2}{3}} \\
u(0) = 0
\end{cases}
\]

cio\`e $ \Omega = \mathbb{R} \times \mathbb{R}^{+}_{0} $, $ t_{0} = 0 $, $ v = 0 $, $ f(z) = 3 z^{\frac{2}{3}} $. Si osserva che
\begin{itemize}
\item $ u_{1}(t) \equiv 0 $
\item $ u_{2}(t) = t^{3} $
\end{itemize}
sono soluzioni del problema considerato. Si noti inoltre che

\[ \lim\limits_{z \to 0^{+}} f'(z) = \lim\limits_{z \to 0^{+}} 2 z^{-\frac{1}{3}} = \infty \]

\subsection{Un criterio di unicit\`a (per $ k = 1 $)}
Nelle ipotesi di \refsec{3.1} si supponga che $ k = 1 $. Se $ [f(t, z_{1}) - f(t, z_{2})] \cdot [z_{1} - z_{2}] \le l \vert z_{1} - z_{2} \vert^{2}, \forall\ (t, z_{1}), (t, z_{2}) \in \Omega $, dove
\begin{itemize}
\item $ \cdot $ \`e il prodotto scalare in $ \mathbb{R}^{d} $
\item $ l \in \mathbb{R} $
\end{itemize}

allora due soluzioni $ u_{1}, u_{2}: I \to \mathbb{R} $ di $ (\ref{eq:PVI}) $ coincidono in $ \mathcal{C}(I, \mathbb{R}^{d}) $.

\begin{proof}
$ \forall\ t \in I $ si ha

\begin{dmath*}
{ \frac{\mathrm{d}}{\mathrm{d}t} \left[ \frac{1}{2} \vert u_{1}(t) - u_{2}(t) \vert^{2} \right] } = { \left( u_{1}(t) - u_{2}(t) \right) \cdot \left( u_{1}' (t) - u_{2}' (t) \right) } = \\ { [ f \left( t, u_{1}(t) \right) - f \left( t, u_{2}(t) \right) ] \cdot [u_{1}(t) - u_{2}(t)] } \le { l \vert u_{1}(t) - u_{2} (t) \vert^{2} }
\end{dmath*}

Quindi

\[ d(t) \defeq \frac{1}{2} e^{-2lt} \vert u_{1}(t) - u_{2}(t) \vert^{2} \]

soddisfa

\[ d'(t) \le -l e^{-2lt} \vert u_{1}(t) - u_{2}(t) \vert^{2} + e^{-2lt} l \vert u_{1}(t) - u_{2}(t) \vert^{2} = 0 \]

da cui

\[ d(s) = \underbrace{d(0)}_{= 0} + \int\limits_{0}^{s} \underbrace{d'(t)}_{\le 0} \, \mathrm{d}t \]

e quindi, in $ I $

\[ d(t) \equiv 0 \implies u_{1} \equiv u_{2}  \]
\end{proof}

\section{Metodi numerici e condizionamento}
\subsection{Metodi numerici}

Un metodo numerico \`e una "mappa" del tipo

\begin{center}
(problema continuo, parametro) $ \mapsto $ problema discreto
\end{center}

Ad esempio in \refsec{1} abbiamo associato al problema continuo $ \ref{eq:PVI} $ e al parametro $ N \in \mathbb{N} $ il problema discreto $ (\ref{eq:EE}) $ con $ t_{n} = t_{0} + n \tau $ e $ \tau = \frac{T - t_{0}}{N} $. Si noti che i $ t_{i} $ sono tutti equidistanti. Se non lo fossero, come in metodi che verranno affrontati successivamente, al posto di un parametro si dovrebbe inserire un vettore di punti che formano la griglia dei tempi. Anche questa soluzione non \`e generale poich\'e alcuni metodi prevedono un ordine preciso di uso dei tempi $ t_{i} $. Per ora saranno considererati solo metodi con griglia equidistante.

I metodi "numerici" sono un modello dei metodi "computazionali".

\subsection{Convergenza}
Per valutare la bont\`a  di un metodo numerico \`e interessante bilanciare costo e qualit\`a: nel caso di \refsec{1}, ad esempio, sarebbe il bilanciamento di $ N $, dato che il numero dei passi \`e uguale al numero di valutazioni della $ f $ e quindi \`e approssimabile al costo computazionale, oppure  $ \max\limits_{n = 0, \dotsc, N} \vert u(t_{n})  - U_{n} \vert $. In particolare ci interessano le stime del tipo $ \max\limits_{n = 0, \dotsc, N} \vert u(t_{n})  - U_{n} \vert \le C N^{-p} $, con $ p > 0 $, che qualifica anche la velocit\`a di convergenza.

\subsection{Buona posizione e buon posizionamento}
I problemi continui e discreti dovrebbero essere ben posti, cio\`e, secondo Hadamard:
\begin{enumerate}
\item deve esistere almeno una soluzione;
\item deve esistere al pi\`u una soluzione;
\item la soluzione deve dipendere in modo continuo dai dati.
\end{enumerate}

Nel caso di $ (\ref{eq:PVI}) $ i dati sono $ f $ e $ v $. Inoltre, il problema discreto che viene "risolto" sul computer con aritmetica finita dovrebbe essere in particolare ben condizionato, cio\`e:
\begin{enumerate}
\item[4.] piccole perturbazioni nei dati producono piccole perturbazioni nella soluzione.
\end{enumerate}

Poich\'e il problema discreto discende dal problema continuo e si spera che lo "diventi" per $ N \to +\infty $, anche il problema continuo dovrebbe essere ben condizionato. Si noti che il punto 2 pu\`o essere riscritto nel seguente modo:
\begin{enumerate}
\item[2b.] nessuna perturbazione nei dati, quindi nessuna perturbazione nella soluzione.  
\end{enumerate}

In altre parole, prima di comprendere il punto 3 bisogna comprendere il punto 2.

\section{Buon condizionamento grazie alla CLU}

\subsection{Stabilit\`a}
Siano $ t_{0}, T \in \mathbb{R} $ come sempre con $ t_{0} < T $, $ f: [t_{0}, T] \times \mathbb{R}^{d} \to \mathbb{R}^{d} : \forall\ t \in [t_{0}, T], z_{1}, z_{2} \in \mathbb{R}^{d} $ si ha:

\[
[ f(t, z_{1}) - f(t, z_{2}) ] \cdot [z_{1} - z_{2}] \leq l \cdot \Vert z_{1} - z_{2} \Vert^{2}
\]

per qualche $ l \in \mathbb{R} $. Siano poi $ v \in \mathbb{R}^{d} $ e $ u, \tilde{u} \in \mathcal{C}^{1}([t_{0}, T], \mathbb{R}^{d}) $ soluzioni rispettivamente delle due equazioni differenziali:

\[
\begin{cases}
u' = f(\cdot, u) \\
u(t_{0}) = v
\end{cases}
\]

\[\begin{cases}
\tilde{u}' = f(\cdot, u) + \delta \\
\tilde{u}(t_{0}) = v + \rho = \tilde{v}
\end{cases}
\]

nell'intervallo $ [t_{0}, T] $ dove $ \delta: [t_{0}, T] \to \mathbb{R}^{d} $ e $ \rho \in \mathbb{R}^{d} $. Allora:
 
\[
\vert u(t) - \tilde{u}(t) \vert \leq e^{l(t - t_{0})} \vert \rho \vert + \int\limits_{t_{0}}^{t} e^{l(t - s)} \vert \delta(s) \vert \, \mathrm{d}s \]

Se $ l < 0 $ si ha una decrescita esponenziale, se $ l > 0 $ una crescita. Si potrebbe raffinare $ l $ e farla dipendere da $ t $.

\begin{proof}
La presenza di $ \delta $ suggerisce di considerare $ \Vert u - \tilde{u} \Vert $ al posto di $ \Vert u -  \tilde{u} \Vert^{2} $. Per evitare problemi di derivabilit\`a in $ 0 $, si consideri dapprima $ \sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}} $ e poi il limite $ \varepsilon \to 0 $.

Seguendo 1.3.3 si osserva:	% TODO: cosa sarebbe 1.3.3?

\begin{dmath*}
{ \frac{\mathrm{d}}{\mathrm{d} t} [\sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}}] } = { \frac{1}{2} \frac{2 \cdot (u - \tilde{u}) \cdot (u' - \tilde{u}')}{\sqrt{\varepsilon^{2} + \Vert u - \tilde{u}\Vert^{2}}} } = { \frac{(u - \tilde{u}) \cdot (f(\cdot, u) - f(\cdot, \tilde{u}) - \delta)}{\sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}}} } \leq \\ { l \cdot \frac{\Vert u - \tilde{u} \Vert^{2}}{\sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}}} + \vert \delta \vert \frac{\Vert u - \tilde{u} \Vert^{2}}{\underbrace{\sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}}}_{\leq 1}} } \leq { l \cdot \frac{\Vert u - \tilde{u} \Vert^{2}}{\sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}}} + \vert \delta \vert }
\end{dmath*}
 
Come prima si consideri la funzione:

\[ d_{\varepsilon}(t) \defeq e^{-l(t - t_{0})} \sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}} \]

e si osserva che: 
 
\begin{dmath*}
{ d_{\varepsilon}'(t) } \leq { - l e^{-l(t - t_{0})} \sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}} + e^{-l(t - t_{0})} \left[ l \cdot \frac{\Vert u - \tilde{u}\Vert^{2}}{\sqrt{\varepsilon^{2} + \Vert u - \tilde{u} \Vert^{2}}} + \vert \delta \vert \right] } = { e^{-l(t - t_{0})} \left[ \vert \delta \vert + l \frac{-\varepsilon^{2} - [u - \tilde{u}] + [u - \tilde{u}]^{2}}{\sqrt{\varepsilon^{2} + \vert u - \tilde{u} \vert^{2}}} \right] } \leq { e^{-l(t - t_{0})} \left[ \vert \delta \vert + \vert l \vert \cdot \varepsilon \frac{\varepsilon}{\sqrt{\varepsilon^{2} + \vert u - \tilde{u} \vert^{2}}} \right] } \leq { e^{-l(t - t_{0})} [ \vert \delta \vert + \varepsilon \vert l \vert ] }
\end{dmath*}

Di conseguenza:

\[
d_{\varepsilon} (t) = d_{\varepsilon}(t_{0}) + \int\limits_{t_{0}}^{t} d_{\varepsilon}'(s)\, \mathrm{d}s = d_\varepsilon(t_{0}) + \int\limits_{t_{0}}^{t} e^{-l(s - t_{0})} \vert \delta(s) \vert\, \mathrm{d}s + \varepsilon \cdot \vert l \vert \cdot \int\limits_{t_{0}}^{t} e^{-l(s - t_{0})}\,\mathrm{d}s
\]
 
E considerando $ \varepsilon \to 0 $ si ottiene:

\[ d_{0}(t) \leq d_{0}(t_{0})+ \int\limits_{t_{0}}^{t} e^{-l(s - t_{0})} \cdot \vert \delta(s) \vert\, \mathrm{d}s + 0 \] 

ovvero che:

\[ e^{-l(t - t_{0})} \cdot \Vert u - \tilde{u} \Vert \leq \vert \rho \vert + \int\limits_{t_{0}}^{t} e^{-l(s - t_{0})} \cdot \vert \delta(s) \vert\, \mathrm{d}s \]

da cui la tesi moltiplicando per $ e^{l(t - t_{0})} $.
\end{proof}

\section{Residuo e consistenza}
\subsection{Poligono di Eulero}

La soluzione discreta del metodo di Eulero esplicito fornisce valori solo sulla griglia generale $ t_{n} = t_{n-1} + \tau_{n} $ con $ n = 0, 1, \dotsc, N - 1 $ e $ \tau = 0, \dotsc, T_{N - 1} > 0 $ tali che $ \sum\limits_{k = 0}^{N - 1} \tau_{n} = T - t_{0} $. Quindi non pu\`o essere vista come perturbazione di $ u $.

Per rimediare si passa al poligono di Eulero: una funzione $ U: [t_{0}, T] \to \mathbb{R}^{d} $ definita da

\begin{dmath*}
U(t) = \frac{t - t_{n - 1}}{\tau_{n}} U_n + \frac{t_{n} - t}{\tau_{n}} U_{n - 1} = U_{n - 1} + (t - t_{n - 1}) \frac{U_{n} - U_{n-1}}{\tau_{n}} = U_{n - 1} + (t - t_{n - 1}) f(t_{n - 1}, U_{n - 1})
\end{dmath*}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
      \draw[->] (-1, 0) -- (7,0) node[right] {$ t $};
      \draw[->] (0, -1) -- (0, 3.5) node[above] {$ U(t) $};
      \draw (0, 0.8) -- (2, 2) -- (4, 1.2) -- (6, 2.8) -- (6.5, 2.6);

      \node [below left] at (0, 0) {$ t_{0} $};
      \draw[dashed] (2, 2) -- (2, 0) node[below] {$ t_{1} $};
      \draw[dashed] (4, 1.2) -- (4, 0) node[below] {$ t_{2} $};
      \draw[dashed] (6, 2.8) -- (6, 0) node[below] {$ t_{3} $};
      
      \draw [fill] (2,2) circle (1pt) node[above] {$ f(t_{0}, U_{0}) $};
      \draw [fill] (4,1.2) circle (1pt) node[below] {$ f(t_{1}, U_{1}) $};
      \draw [fill] (6,2.8) circle (1pt) node[above] {$ f(t_{2}, U_{2}) $};
\end{tikzpicture}

\caption{Esempio di poligono di Eulero}
\end{center}
\end{figure}

\subsection{Residuo del poligono di Eulero}
Dato il poligono di Eulero, si definisce residuo la quantit\`a:

\[ R(t) \defeq u'(t) - f(t, U(t)) \]

per $ t \in (t_{n - 1}, t_{n}) $ e si osserva che $ u'(t) = f(t, U(t)) + R(t) $, di conseguenza nelle ipotesi del teorema precedente si ha:

\[ \Vert u(t) - U(t) \Vert \leq \int\limits_{t_{0}}^{t} e^{l(t - s)} \vert R(s) \vert \, \mathrm{d}s \] 

dove:

\[
\int\limits_{t_{n}}^{t_{n + 1}} e^{l(t - s)} \vert R(s) \vert \, \mathrm{d}s = \int\limits_{t_{n}}^{t_{n + 1}} e^{l(t - s)} [f(t_{n}, U_{n}) - f(s, U(s))] \, \mathrm{d}s
\]

\`e calcolabile (modulo integrazione numerica) pu\`o essere associato al passo $ n $.	% TODO: frase non chiara

\subsection{Consistenza del passo}
Una controparte del residuo pu\`o essere:

\[
\frac{U(t_{n - 1}) - u(t_{n})}{\tau_{n}} - f(t_{n}, U(t_{n})) = \frac{\varepsilon(t_{n}, U(t_{n}),\tau_{n})}{\tau_{n}} \eqdef \frac{\varepsilon_{n}}{\tau_{n}}
\]

Infatti esso misura per quanto la soluzione esatta non soddisfa la soluzione discreta mentre il residuo misura per quanto l'approssimazione $ U $ non soddisfa l'equazione continua supponendo che il problema discreto sia ben condizionato oppure vale una controparte di $ 1.5.1 $, ci si pu\`o aspettare una stima dell'errore in termini di $ \frac{\varepsilon_{n}}{\tau_{n}} $, con $ n = 0, \dotsc, N - 1$. Si noti che grazie a $ u' = f(t, u) $ si ha:	% TODO: cosa sarebbe 1.5.1?

\[ \frac{\varepsilon(t, U(t), \tau)}{\tau} = \frac{U(t + \tau) - U(t)}{\tau_{n}} - f(t, U(t)) \to 0 \]

per $ \tau \to 0 $ poich\'e u \`e derivabile in $ \tau $.

\chapter{Consistenza e regolarit\`a}
\setcounter{section}{6}
\section{Preliminari}
\subsection{Evoluzione}	\label{section:7.1}
Sia $ \Omega \subseteq \mathbb{R} \times \mathbb{R}^{d} $, $ f: \Omega \to \mathbb{R}^{d} $ tale che $ \forall\ (t_{0}, v) \in \Omega $ il problema

\begin{equation}	\label{eq:probevol}
\begin{cases}
u' = f(\cdot, u) \\
u(t_{0}) = v
\end{cases}
\end{equation}

ha un'unica soluzione massimale $ u(t_{0}, v) : [t_{0}, T(t_{0}, v)] \to \mathbb{R}^{d} $ derivabile in $ t_{0} $. Qui massimale significa che se $ \tilde{u} : [t_{0}, \tilde{T}] \to \mathbb{R}^{d} $ \`e un'altra soluzione allora $ \tilde{T} < T(t_{0}, v) $ e $ \tilde{u} = u $ su $ [t_{0}, \tilde{T}) $

Si pone quindi, $ \forall\ t \in [t_{0}, T(t_{0}, v)] $

\[ \varphi(t, t_{0}) v = u_{(t_{0}, v)} (t) \]

chiamata evoluzione di $ f $.

\begin{oss}
\noindent
\begin{enumerate}
\item $ \varphi(t_{0}, t_{0}) v = v $;
\item Se $ t_{0} \le t_{1} \le t_{2} \le t_{3} < T(t_{0}, v) $ allora $ \varphi(t_{3}, t_{2}) \varphi(t_{2}, t_{1}) v = \varphi(t_{3}, t_{1}) v $;
\item $ \lim\limits_{\tau \to 0^{+}} \frac{\mathrm{d}}{\mathrm{d} \tau} \varphi (t + \tau, t)v = \lim\limits_{\tau \to 0^{+}} \frac{\varphi(t + \tau, t)v - \varphi(t, t)v}{\tau} = f(t, v) $
\end{enumerate}
\end{oss}

\subsection{Metodi a un passo}

Siano $ \Omega, f $ e $ \varphi $ come nel capitolo precedente. Un metodo ad un passo associa a $ f $ una funzione incremento $ F = F(t, z, \tau), \forall\ (t, z) \in \Omega, \tau \in [0, \tau(t, z)] $ con $ \tau(t, z) > 0 $. Posto

\[ \Phi (t_{1}, t_{2}) v = v + (t_{2} - t_{1}) F(t_{1}, v, t_{2} - t_{1}) \]

una soluzione approssimata di $ (\ref{eq:probevol}) $ in \refsec{7.1} sulla griglia

\[ t_{0} < t_{1} < \dotsb < t_{N} \]

\`e data da 

\[
\begin{cases}
U_{n+1} = \Phi(t_{n + 1}, t_{n}) U_{n} \\
U_{0} = v
\end{cases}
\]

\begin{oss}
\noindent
\begin{enumerate}
\item $ \Phi(t_{n+1}, t_{n}) v $ vuole approssimare $ \varphi(t_{n + 1}, t_{n}) $
\item Per il calcolo di $ U_{N} $ non \`e necessario memorizzare $ U_{0}, \dotsc, U_{N - 2} $
\item $ \Phi (t_{0}, t_{0}) v = v $ ma in generale non si ha, per $ 0 \le n \le k \le l \le N $

\[ \Phi(t_{l}, t_{k}) \Phi(t_{k}, t_{n}) v  = \Phi(t_{l}, t_{n}) v \]
\end{enumerate}
\end{oss}

\begin{es}
Per il metodo di Eulero esplicito si ha, con $ (t, z) \in \Omega, T > 0 $

\[ F(t, z, \tau) = f(t, z) \]
\end{es}

\subsection{Teorema di Taylor}

Sia $ I $ intervallo, $ r \in \mathbb{N} $ e $ u \in \mathcal{C}^{r} (I, \mathbb{R}^{d}) = \{ w : I \to \mathbb{R}^{d} : w^{(0)}, \dotsc, w^{(r)} \text{ esistono e sono continue in } I \} $. Allora

\[ \underbrace{u(s)}_{\in I} = \sum\limits_{k = 0}^{r - 1} \frac{u^{(k)}(t)}{k!} (s - t)^{k} + \frac{1}{(k - 1)!} \int\limits_{t}^{s} u^{(r)} (\xi) (s - t)^{r} \, \mathrm{d}\xi \]

\begin{proof}
Analisi
\end{proof}

\subsection{Simboli di Landau}

Sia $ f: [0, \tau^{\ast}] \to \mathbb{R}^{d} $ e $ b: [0, \tau^{\ast}] \to \mathbb{R}^{\ast} = \mathbb{R} \setminus \{ 0 \} $.

\begin{itemize}
\item $ f = O (b) $ per $ \tau \to 0 $ se $ \exists\ C \ge 0, \tau_{0} \in (0, \tau^{\ast}) : \vert f(\tau) \vert \le C b(\tau) $
\item $ f = o(b) $ per $ \tau \to 0 $ se $ \lim\limits_{\tau \to 0^{+}} \frac{\vert f(\tau) \vert}{b(\tau)} = 0 $
\end{itemize}

\begin{es}
\noindent
\begin{itemize}
\item $ u $ continua in $ t \implies u(t + \tau) = u(t) + o(1) $ per $ \tau \to 0 $;
\item $ u $ lipschitziana $ \implies u(t + \tau) = u(t) + O(\tau) $;
\item $ u $ \`e derivabile in $ t \implies u(t + \tau) = u(t) + u'(t) \tau + o(\tau) $
\end{itemize}
\end{es}

\section{Consistenza di metodi a un passo}
\subsection{Definizione di consistenza}	\label{section:8.1}

Un metodo ad un passo con $ \Phi $ e $ F $ si dice consistente per $ u' = f(\cdot, u) $ se una delle seguenti condizioni equivalenti \`e soddisfatta:

\begin{enumerate}
\item $ \forall\ (t, z) \in \Omega, \lim\limits_{\tau \to 0^{+}} \frac{\mathrm{d}}{\mathrm{d} \tau} \Phi(t + \tau, t)z = f(t, z) $
\item $ \forall\ (t, z) \in \Omega, \lim\limits_{\tau \to 0} F(t, z, \tau) = f(t, z) $
\item $ \forall\ (t, z) \in \Omega, \varepsilon(t, z, \tau) \defeq \varphi(t + \tau, t)z - \Phi(t + \tau, t)z = o(\tau) $ per $ \tau \to 0 $
\end{enumerate}

Si osserva che $ \varepsilon(t, z, \tau) \defeq \varphi(t + \tau, t)z - [z + \tau F(t, z, \tau)] $ misura per quanto la soluzione esatta non \`e una soluzione del metodo.

\begin{proof}
\noindent
\begin{itemize}
\item[$ 1. \implies 2. $] $ \lim\limits_{\tau \to 0} F(t, z, \tau) = \lim\limits_{\tau \to 0} \frac{\Phi (t + \tau, t)z - z}{\tau} = \lim\limits_{\tau \to 0^{+}} \Phi(t + \tau, t)z \stackrel{1.}{=} f(t, z) $
\item[$ 2. \implies 3. $] $ \varepsilon(t, \tau, z) = \varphi(t + \tau, t) z - \Phi(t + \tau, t) z = z + \int\limits_{t}^{t + \tau} f(s, \varphi(t + s)) \, \mathrm{d}s - {[z + \tau F(t, z, \tau)]} = \int\limits_{t}^{t + \tau} f(s, \varphi(t + s)) - F(t, z, \tau) \, \mathrm{d}s = {\underbrace{\int\limits_{t}^{t + \tau} \underbrace{f(s + t, \underbrace{\varphi(t + s, t)z}_{= z + o(1) \text{ per } \tau \to 0}) - f(t, z)}_{= o(1)} \, \mathrm{d}s}_{= o(\tau)}} + {\underbrace{\int\limits_{t}^{t + \tau} \underbrace{f(t, z) - F(t, z, \tau)}_{= o(1) \text{ per } \tau \to 0} \, \mathrm{d}s}_{= o(\tau)}} $
\item[$ 3. \implies 1. $] $ \lim\limits_{\tau \to 0} \frac{\mathrm{d}}{\mathrm{d} \tau} \Phi(t + \tau, t)z = \lim\limits_{\tau \to 0} \frac{\Phi(t + \tau, t)z - z}{\tau} = \lim\limits_{\tau \to 0} \frac{\varphi(t + \tau, t)z - z}{\tau} - \frac{\overbrace{\varepsilon(t , z, \tau)}^{o(\tau)}}{\tau} = f(t, z) + 0 $
\end{itemize}
\end{proof}

\subsection{Consistenza del metodo di Eulero esplicito}	\label{section:8.2}
Dato $ (t, z) \in \Omega $, si scrive 

\[ u(t + \tau) = \Phi(t + \tau, t) z \]

Si ha, per $ \tau \to 0 $, che $ \varepsilon_{EE} (t, z) = u(t + \tau) - [u(t) + \tau f(t, u(t))] = u(t + \tau) - u(t) - \tau u'(t) = \int\limits_{t}^{t + \tau} u'(s) \, \mathrm{d}s - \tau u'(t) = \underbrace{\int\limits_{t}^{t + \tau} \underbrace{u'(s) - u'(t)}_{= o(1) \text{ se $ u $ continua in } t} \, \mathrm{d}s}_{O(\vert s - t \vert) = O(\tau^{2})} = o(\tau) $

\section{Un metodo di ordine superiore}
\subsection{Un ordine di pi\`u per $ \varepsilon $. L'errore di consistenza}
In \refsec{8.2} si \`e osservato che l'errore di consistenza di Eulero esplicito \`e di $ O(\tau^{2}) $ per $ \tau \to  0$ se la soluzione di $ u'= f(\cdot u) $ \`e due volte derivabile con continuit\`a . Si cerca di derivare un metodo per cui l'errore di consistenza pu\`o soddisfare $ O(\tau^{3}) $ per $ \tau \to 0 $. A questo scopo si osserva che

$ u(t + h) - u(t - h) = u(t) + h u'(t) + \frac{1}{2} h^{2} u''(t) + \frac{1}{6} h^{3} u^{(3)} (t^{+}) - [u(t) - h u'(t) + \frac{1}{2} h^{2} u''(t) - \frac{1}{6} h^{3} u^{(3)} (t^{-})] = 2h u'(t) + O(t^{3}) $ 

ovvero

\begin{equation}
u'(t) = \frac{u(t + h) - u(t - h)}{2h} + O(h^{3})
\end{equation} 

Sia $ t_{0} < t_{1} < \dotsb < t_{N} = T $ uan griglia e $ \tau_{n} = t_{n + 1} - t_{n} $, con $ n = 0, \dotsc, N - 1 $. Ponendo $ \tau_{n} = \frac{h}{2} $ conduce a $ \frac{u(t_{n + 1}) - u(t_{n})}{\tau_{n}} = u' \left( t_{n} + \frac{\tau_{n}}{2} \right) + O(\tau_{n}^{2}) = f(t_{n} + \frac{\tau_{n}}{2}, u \left( t_{n} + \frac{\tau_{n}}{2} \right)  + O(\tau_{n}^{2}) $. Il problema \`e che $ t_{n} + \frac{\tau_{n}}{2} $ non \`e un punto della griglia, quindi approssimando $ u \left( t_{n} + \frac{\tau_{n}}{2} \right) $ con il metodo di Eulero esplicito con passo $ \frac{\tau_{n}}{2} $ si ottiene 

\[ u \left( t_{n} + \frac{\tau_{n}}{2} \right) = u(t_{n}) + \frac{\tau_{n}}{2} f(t_{n}, u(t_{n})) + O(\tau_{n}^{2}) \]

e, supponendo che $ f = f(t, z) $ \`e Lipschitz rispetto a $ z $, si ottiene

\[ f \left( t_{n} + \frac{\tau_{n}}{2}, u \left(t_{n} + \frac{\tau_{n}}{2} \right) \right) = f(t_{n} + \frac{\tau_{n}}{2}, u(t_{n} + \frac{\tau_{n}}{2} f(t_{n}, u(t_{n}) ) + O(\tau_{n}^{2})  \]

Riassumendo, si ha sotto le ipotesi fatte, per $ \tau \to 0 $

\[ \frac{u(t_{n + 1}) - u(t_{n})}{2 \tau_{n}} = f \left( t_{n} + \frac{\tau_{n}}{2}, u(t_{n}) + \frac{\tau_{n}}{2} f(t_{n}, u(t_{n}) \right) + O(\tau_{n}^{2} \]

Questo suggerisce il cosiddetto metodo di Eulero modificato

\[
\begin{cases}
U_{0} = v \\
U_{n + 1} = U_{n} + \tau f \left( t_{n} + \frac{\tau_{n}}{2}, U_{n} + \frac{\tau_{n}}{2} f(t_{n}, U_{n}) \right)	 & n = 0, \dotsc, N - 1
\end{cases}
\]

Si noti che qui la funzione $ f $ viene valutato due volte per passo. 

\subsection{Ordine sperimentale dell'errore di Eulero modificato}
In un problema dove $ f $ \`e Lipschitz rispetto a  $ z $ e $ u $ tre volte derivabile con continuit\`a si osserva effettivamente che nel tempo finale $ T $

\[ \vert U_{n} - u(T) \vert \approx \tau^{2} = \frac{1}{N^{2}} \]

\begin{center}
\begin{tabular}{c|c|c}
	Metodo & Costo & Errore \\
\hline
	E. E. & $ N $ & $ \approx \frac{1}{N} $ \\
\hline
	E. M. & $ 2N $ & $ \approx \frac{1}{4N^{2}} $ \\
\end{tabular}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%											Sezione ricopiata
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Velocit\`a di convergenza e dipendenza dalla regolarit\`a}
\subsection{Eulero esplicito vs. Eulero modificato}

Nella seguente tabella viene mostrato il costo in termini di valutazion edella $ f $ dei metodi di Eulero esplicito ed Eulero modificato per raggiungere un dato livello di errore relativo (\refsec{9.3})	% TODO: NON ESISTE 9.3!

\begin{tabular}{c|c|c|c|c}
	Tolleranza & $ 10\% $ & $ 1\% $ & $ 0.1\% $ & $ 0.01\% $ \\
\hline
	E.E. & 8 & 64 & 512 & 8192 \\
\hline
	E.M. & 2 & 8 & 32 & 128 \\
\hline
	Raddoppio & 4 & 8 & 16 & 64 \\
\end{tabular}

Questo viene illustrato anche nel bilancio qualit\`a-costo in scala $ \log-\log $

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-1, 0) -- (5, 0) node[right] {$ \log N $};
	\draw[->] (0, -5) -- (0, 1) node[above] {$ \log \mathop{err} $};
	\filldraw (1, 0) circle (1pt) node[above] {$ 10^{1} $};
	\filldraw (2, 0) circle (1pt) node[above] {$ 10^{2} $};
	\filldraw (3, 0) circle (1pt) node[above] {$ 10^{3} $};
	\filldraw (4, 0) circle (1pt) node[above] {$ 10^{4} $};
	\filldraw (0, -1) circle (1pt) node[left] {$ 10^{-1} $};
	\filldraw (0, -2) circle (1pt) node[left] {$ 10^{-2} $};
	\filldraw (0, -3) circle (1pt) node[left] {$ 10^{-3} $};
	\filldraw (0, -4) circle (1pt) node[left] {$ 10^{-4} $};
	\draw (1, 0) -- (4.5, -3.5);
	\draw (1, 0) -- (3.25, -4.5);
\end{tikzpicture}

\caption{$ \mathop{err} \approx C N^{-p} \iff \log \mathop{err} \approx \log C - p \log N $}
\end{center}
\end{figure}

\subsection{Impatto della regolarit\`a}
Si consideri il problema

\[
\begin{cases}
u' = t^{p} u & \text{ in } (0, 1) \\
u(1) = \exp \left( \frac{1}{1 + p} \right) 
\end{cases}
\]

con $ p > -1 $. La soluzione \`e $ u(t) = \exp \left( \frac{t^{p + 1}}{p + 1} \right) $ per $ t \ge 0 $. Posto $ f(t, z) = t^{p} z $, con $ z \in \mathbb{R} $ e $ t > 0 $ si ha il seguente grafico:

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-1, 0) -- (2, 0) node[right] {$ z $};
	\draw[->] (0, -1) -- (0, 3) node[right] {$ f(2, z) $};
	\filldraw (1, 0) circle (1pt) node[below] {$ 1 $};
	\filldraw (0, 0) circle (1pt) node[below left] {$ 0 $};

    \draw[domain=0:1, dashed, smooth] plot (\x, {2*\x^0.4});
    \draw[domain=0.05:1, smooth] plot (\x, {2*\x^(-0.1)});
    \draw[domain=0:1, dotted, smooth] plot (\x, {2*\x});
    \draw[domain=0:1, smooth] plot (\x, {2*\x^10});
\end{tikzpicture}

\caption{Grafici di $ f(2, z) $ con $ p = 1 $ (puntinato), $ p = 0.4 \in (0, 1) $ (tratteggiato), $ p = 10 \gg 1 $ (grafico inferiore) e $ p = -0.7 \in (-1, 0) $ (grafico superiore)}
\end{center}
\end{figure}

Si osserva numericamente che:
\begin{enumerate}
\item con l'aumentare di $ p \gg 1 $ l'ordine del metodo si osserva pi\`u tardi, cio\`e per $ N $ grandi;
\item per $ p \in (0, 1) $ il metodo di Eulero modificato mostra solo un ordine di convergenza di $ p + 1 < 2 $. Una "spiegazione" \`e data dalla barriera dell'errore di consistenza dell'ultimo passo $ \approx \tau^{3} \tau^{p - 2} = \tau^{p + 1} $;
\item per $ p \in (-1, 0) $ entrambi i metodi mostrano solo l'errore di convergenza $ 1 + p < 1 $. Una "spiegazione" \`e data da $ \tau^{2} \tau^{p - 1} = \tau^{p + 1} $.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
%										Sezione 11 e 12 ricopiate
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Regolarit\`a di soluzioni}
\subsection{Spazi $ \mathcal{C}^{r}(A), A \subseteq \mathbb{R}^{n} $}

Sia $ A \subseteq \mathbb{R}^{n} $ non vuoto e aperto. Dato un multiindice $ \alpha = (\alpha_{1}, \dotsc, \alpha_{n}) \in \mathbb{N}_{0}^{n} $ si scrive $ \partial^{\alpha} g(x) = \frac{\partial^{\vert \alpha \vert}}{\partial^{\alpha_{1}} x_{1} \dotsm \partial^{\alpha_{n}} x_{n}} g(x) $ con $ \vert \alpha \vert = \sum\limits_{i = 1}^{m} \alpha_{i} $ e si pone $ \mathcal{C}^{r}(A, \mathbb{R}^{l}) \defeq \{ g: A \to \mathbb{R}^{d} : \forall\ \vert \alpha \vert < r, \exists\ \partial^{\alpha} g \text{ ed \`e continua} \} $ e $ \mathcal{C}^{\infty} (A, \mathbb{R}^{l}) \defeq \bigcap\limits_{r \in \mathbb{N}} \mathcal{C}^{r} (A, \mathbb{R}^{l}) $.

\subsection{Regolarit\`a superiore}	\label{section:11.2}
Siano $ \Omega \subseteq \mathbb{R} \times \mathbb{R}^{d} $ non vuoto, aperto e connesso, $ f: \Omega \to \mathbb{R}^{d} $, $ t_{0} < T $, $ v \in \mathbb{R}^{d} $, $ (t_{0}, v) \in \Omega $ e sia $ u $ una soluzione di $ (\ref{eq:PVI}) $ in $ [t_{0}, T] $. $ \forall\ r \in \mathbb{N}_{0} \cup \{ \infty \} $ si ha $ f \in \mathcal{C}^{r} (\Omega, \mathbb{R}^{d}) \implies u \in \mathcal{C}^{r + 1} ([t_{0}, T], \mathbb{R}^{d}) $.

\begin{proof}
Si definisce induttivamente 

\[
\begin{cases}
f_{0} (t, z) \defeq f(t, z) \\
f_{k + 1} (t, z) \defeq \partial_{t} f_{(k)} (t, z) + \partial_{z} f_{(k)} (t, z) f(t, z)
\end{cases}
\]

Si osserva che $ u^{(1)} (t) = f(t, u(t)) - f_{(0)} (t, u(t)) $ e quindi $ f \in \mathcal{C}^{0}(\Omega, \mathbb{R}) $, ovvero $ u \in \mathcal{C}^{1}([t_{0}, T], \mathbb{R}^{d}) $. Inoltre se $ r \ge 1 $ si possono derivare (dapprima il secondo membro) $ u^{(2)} (t) = \frac{\mathrm{d}}{\mathrm{d} t} [f_{(0)} (\cdot, u)] (t) = \partial_{t} f_{(0)} (t, u(t)) + \partial_{z} f(t, u(t)) u'(t) = f_{(1)} (t, u(t)) $ da cui $ f \in \mathcal{C}^{1} (\Omega, \mathbb{R}^{d}) $ e quindi $ u \in \mathcal{C}^{2} ([t_{0}, T], \mathbb{R}^{d}) $. Ripetendo con $ (r) $ al posto di $ (1) $ si arriva a $ u^{(r + 1)} (t)  f_{(r)} (t, u(t)) $, da cui la tesi.
\end{proof}

\subsection{Corollario}
Nelle ipotesi di \refsec{11.2} si ha $ \forall\ k = 0, \dotsc, r - 1, u^{(k + 1)} (t) = f_{(k)} (t, u(t)) $.

\section{Ordine di consistenza}
\subsection{Definizione di ordine di convergenza}
Un metodo ad un passo consistente di ordine $ r \in \mathbb{N} $ per $ u' = f(\cdot, u) $ con $ f \in \mathcal{C}^{r} (\Omega, \mathbb{R}^{d}) $ implica che $ \forall\ (t, z) \in \Omega, \varepsilon(t, z, \tau) = \varphi (t + \tau, t)z - \Phi(t + \tau, t) = \mathcal{O}(\tau^{r + 1}) $ con $ \tau \to 0 $.

\subsection{Condizioni sufficienti}	\label{section:12.2}
Un metodo ad un passo \`e consistente di ordine $ r $ per $ u' = f(\cdot, u) $ se $ f \in \mathcal{C}^{r}(\Omega, \mathbb{R}^{d}) $ implica che $ \forall\ (t, z) \in \Omega, \exists\ \tau^{\ast} > 0 : F(t, z, \cdot) \in \mathcal{C}^{r}([0, \tau^{\ast}], \mathbb{R}^{d}) $ se vale una delle seguenti condizioni equivalenti:
\begin{enumerate}
\item $ \forall\ (t, z) \in \Omega, k = 1, \dotsc, r, \frac{\mathrm{d^{k}}}{\mathrm{d} \tau^{k}} \big\vert_{\tau = 0} \Phi(t + \tau, t)z = f_{(k + 1)}(t, z) $;
\item $ \forall\ (t, z) \in \Omega, k = 1, \dotsc, r, \partial_{\tau} F^{k} (t, z, 0) = \frac{1}{k + 1} f_{(k)} (t, z) $.
\end{enumerate}

\begin{proof}
\noindent
\begin{itemize}
\item Sia $ f \in \mathcal{C}^{r}(\Omega, \mathbb{R}^{d}) $, in virt\`u di \refsec{11.2} si ha $ \varphi(t + \cdot, t) z \in \mathcal{C}^{r + 1} ([0, \tau^{\ast}], \mathbb{R}^{d}) $ per qualche $ \tau^{\ast} > 0 $. Derivando $ \Phi(t + \cdot, t) z $ si ha $ \frac{\mathrm{d}}{\mathrm{d} \tau} \Phi(t + \tau, t) z = \frac{\mathrm{d}}{\mathrm{d} \tau} [z + \tau F(t, z, \tau)] = F(t, z, \tau) + \tau \partial_{\tau} F(t, z, \tau) $ grazie alla regola del prodotto. Per induzione si ottiene $ \frac{\mathrm{d}^{k}}{\mathrm{d} \tau^{k}} \Phi(t + \tau, t) z = k \partial_{\tau}^{k - 1} F(t, z, \tau) + \tau \partial_{\tau}^{k} F(t, z, \tau) $. Questo implica che $ 1. \iff 2. $;
\item Usando gli sviluppi di Taylor si ottiene 

\begin{dmath*}
\varepsilon(t, z, \tau) = \varphi(t + \tau, t) z - \Phi(t + \tau, t) = z + \sum\limits_{k = 1}^{r} \frac{1}{k!} f_{(k - 1)} (t, z) \tau^{k} + \mathcal{O}(\tau^{r + 1}) - \left[ z + \tau \left( \sum\limits_{k = 0}^{r - 1} \frac{1}{k!} \partial_{\tau}^{k} F(t, z, 0) \tau^{k} + \mathcal{O}(\tau^{k}) \right) \right] = \sum\limits_{k = 1}^{r} \frac{1}{(k - 1)!} \partial_{\tau}^{k - 1} F(t, z, 0) \tau^{k - 1} = \sum\limits_{k = 1}^{r} \left[ \frac{1}{(k - 1)!} f_{(k - 1)} (t, z) - \frac{1}{(k - 1)!} \partial_{\tau}^{k - 1} F(t, z, 0) \right] \tau^{k} = 0
\end{dmath*}

Grazie a $ 2. $ si ha che il tutto \`e $ \mathcal{O}(\tau^{r + 1}) $.
\end{itemize}
\end{proof}

\section{Metodi di Runge-Kutta}
\subsection{Definizione}
Un metodo di Runge-Kutta (RK) a $ s \in \mathbb{N} $ stadi \`e determinato da:

\begin{itemize}
\item $ A = (a_{ij})_{i, j = 1, \dotsc, s} \in \mathbb{R}^{s \times s} $
\item $ b = (b_{1}, \dotsc, b_{s})^{T} \in \mathbb{R}^{s} $
\item $ c = (c_{1}, \dotsc, c_{s})^{T} \in \mathbb{R}^{s} $ 
\end{itemize}

e associa a una funzione $ f: \Omega \subseteq \mathbb{R} \times \mathbb{R}^{d} \to \mathbb{R}^{d} $ la funzione

\[ F(t, z, \tau) = \sum\limits_{i = 1}^{s} b_{i} k_{i} \]

dove gli stadi $ k_{1}, \dotsc, k_{s} \in \mathbb{R}^{d} $ sono "la" soluzione del sistema non lineare

\begin{equation}	\label{eq:ki}
k_{i} = f \left( t + c_{i} \tau, z + \tau \sum\limits_{j = 1}^{s} a_{ij} k_{j} \right)
\end{equation}

con $ i = 1, \dotsc, s $. I parametri si rappresentano solitamente con il cosiddetto schema di Butcher

\begin{center}
\begin{tabular}{c|ccc}
                  &   &       & \\
$ \underline{c} $ &   & $ A $ & \\
				  &   &       & \\
\hline
				  &   & $ \underline{b}^{T} $ & \\
\end{tabular}
\end{center}

Il metodo si dice esplicito se $ a_{ij} = 0 $ per $ j \ge i $, ovvero $ A = \begin{pmatrix}
	     0 &      0 & \ldots &      0 \\
	  \ast & \ddots & \ddots & \vdots \\
	\vdots & \ddots & \ddots &      0 \\
	  \ast & \ldots &   \ast &      0 \\
\end{pmatrix} $. In questo caso, $ (\ref{eq:ki}) $ di riduce a:
\begin{itemize}
\item $ k_{1} = f(t + c_{1} \tau, z) $
\item $ k_{2} = f(t + c_{2} \tau, z + \tau a_{21} k_{1}) $
\item $ k_{s} = f \left( t + c_{s} \tau, z + \tau \sum\limits_{j = 1}^{s-1} a_{sj} k_{j} \right) $
\end{itemize}

\subsection{Esempi}
\begin{enumerate}
\item Eulero esplicito:
\[ F(t, z \tau) = f(t, z) \]

\begin{center}
\begin{tabular}{c|c}
$ 0 $ & $ 0 $ \\
\hline
	  & $ 1 $ \\
\end{tabular}
\end{center}
\item Eulero modificato: 
\[ F(t, z \tau) = f \left( t + \frac{\tau}{2}, z + \frac{\tau}{2} f(t, z) \right) \]

\begin{center}
\begin{tabular}{c|cc}
          $ 0 $ &           $ 0 $ & $ 0 $ \\
$ \frac{1}{2} $ & $ \frac{1}{2} $ & $ 0 $ \\
\hline
                &           $ 0 $ & $ 1 $ \\

\end{tabular}
\end{center}
\item Eulero implicito:
\[ z^{+} = z + \tau f(t + \tau, z^{+}) \iff \frac{z^{+} - z}{\tau} = \underbrace{f(t + \tau, z^{+})}_{\eqdef k_{1}} \iff k_{1} = f(t + \tau, z + \tau k_{1}) \]

\begin{center}
\begin{tabular}{c|c}
$ 1 $ & $ 1 $ \\
\hline
	  & $ 1 $ \\
\end{tabular}
\end{center}
\end{enumerate}

\subsection{Consistenza}
Un metodo Runge-Kutta definito da $ (A, \underline{b}, \underline{c}) $ \`e consistente $ \forall\ f $ continua se e solo se $ \sum\limits_{i = 1}^{s} b_{i} = 1 $.

\begin{proof}
Essendo un metodo a un passo si pu\`o usare \refsec{8.1} (2). Nel caso di $ \tau = 0 $, $ (\ref{eq:ki}) $ si riduce a

\[ k_{i} = f \left(t + c_{i} 0, z + 0 \sum\limits_{j = 1}^{s} a_{ij} k_{j} \right) = f(t, z) \]

$ \forall\ i = 1, \dotsc, s $ da cui $ F(t, z, 0) = \sum\limits_{i = 1}^{s} b_{i} k_{i} = \left( \sum\limits_{i = 1}^{s} b_{i} \right) f(t, z) $. Se il metodo \'e consistente allora $ F(t, z, 0) = f(t, z) $ e $ f(t, z) \ne 0 $ implica $ \sum\limits_{i = 1}^{s} b_{i} = 1 $. Viceversa, se $ \sum\limits_{i = 1}^{s} b_{i} = 1 $ e se $ k_{i} = k_{i} (t, z) $ sono continui in $ (t, z) $, allora vale \refsec{8.1} (2)  e il metodo definito da $ (A, \underline{b}, \underline{c}) $ \`e consistente.
\end{proof}

\section{Consistenza di ordine superiore di metodi RK}
\subsection{Derivazione delle condizioni}

Prima si derivano delle condizioni in termini dello schema di Butcher a $ s $ stadi. Per semplificare si supponga

\begin{equation}	\label{eq:ci+f}
\begin{cases}
c_{i} = \sum\limits_{j = 1}^{s} a_{ij} \\
f = f(z)
\end{cases}
\end{equation} 

Si vuole usare, per $ k = 0, \dotsc, r - 1 $ (si veda \refsec{12.2} (1.)):

\[ \partial^{k}_{\tau} F(z, 0) \defeq \frac{\partial^{k}}{\partial \tau^{k}} F(z, 0) = \frac{1}{1 + k} f_{(k)} (z) \]

Dapprima si definisce $ \partial^{k}_{\tau} F(z, t)  $ in termini di $ A $, $ b $ e $ f $. Dalla definizione si ha

\[ \partial^{k}_{\tau} F(z, 0) = \sum\limits_{i = 1}^{s} b_{i} \partial^{k}_{\tau} K_{i}(z, \tau) \]

dove $ K_{i} = K_i(z, \tau) $ sono tali che, $ \forall\ i = 1, \dotsc, s $:

\begin{equation}	\label{eq:Ki}
K_{i} = f \left( z + \tau \sum\limits_{j = 1}^{s} a_{ij} K_{j} \right) 
\end{equation}

Per determinare le derivate dei $ K_{i} $ si ricostruisce lo sviluppo di Taylor "intorno" a $ \tau = 0 $ iterativamente. Grazie alla continuit\`a di $ f $ si ha per $ \tau \to 0 $ e $ \forall\ i = 1, \dotsc, s $:

\[ K_{i} (z, \tau) = f(z) + o(1) \]

Usando questa approssimazione in $ (\ref{eq:Ki}) $ e uno sviluppo di Taylor al secondo passaggio si ottiene

\begin{dmath}	\label{eq:Ki2}
K_{i} = f \left( z + \tau \sum\limits_{j = 1}^{s} a{ij} [f(z) + o(1)] \right) = \underbrace{f(z)}_{\in \mathbb{R}^{d}} + \sum\limits_{l = 1}^{d} \underbrace{\partial_{l} f(z)}_{\in \mathbb{R}^{d}} \tau \underbrace{\sum\limits_{j = 1}^{s} a_{ij}}_{= c_{i} \text{ per } (\ref{eq:Ki})} \underbrace{f_{l}(z)}_{\in \mathbb{R}} + o(\tau) = f(z) + \underbrace{c_{i} \tau \sum\limits_{l = 1}^{d} \partial_{l} f(z) f_{l} (z) + o(\tau) }_{\text{che corrisponde a } \approx o(1)}
\end{dmath}

Quindi 

\[ \partial_{\tau} K_{i}(z, 0) = c_{i} \sum\limits_{l = 1}^{d} f_{l}(z) \partial_{l} f(z) \]

Usando l'approssimazione migliorata $ (\ref{eq:Ki2}) $ e facendo uno sviluppo di Taylor si ottiene

\begin{dmath}
K_{i} = f \left( z + \tau \sum\limits_{j = 1}^{s} a_{ij} \left[ f(z) + c_{j} \tau \sum\limits_{l = 1}^{d} f_{l}(z) \partial_{l} f(z) + o(\tau) \right] \right) = f(z) + \sum\limits_{m = 1}^{d} \partial_{m} f(z) \tau c_{i} f_{m}(z) + \sum\limits_{m = 1}^{d} \partial_{m} f(z) \tau^{2} \sum\limits_{j = 1}^{s} a_{ij} c_{j} \sum\limits_{l = 1}^{d} f_{l}(z) \partial_{l} f_{m} (z) + o(\tau^{2}) + \frac{1}{2} \sum\limits_{m, l = 1}^{d} \partial_{lm} f(z) \tau^{2} c_{i}^{2} f_{l}(z) + o(\tau^{2})
\end{dmath}

da cui

\[ \partial_{\tau}^{2} K_{i} (z, 0) = 2 \left( \sum\limits_{j = 1}^{s} a_{ij} c_{j} \right) \sum\limits_{m, l = 1}^{d} f_{l}(z) \partial_{l} f_{m}(z) \partial_{m} f(z) + c_{i}^{2} \sum\limits_{m, l = 1}^{d} f_{l}(z) f_{m}(z) \partial_{lm} f(z) \]	% TODO: controllare

Questo era il primo membro di \refsec{12.2} (2). Ora si consideri il secondo membro. Si ha

\begin{itemize}
\item $ f_{(1)}(z) = \underbrace{\partial_{z}}_{(\partial_{1}, \dotsc, \partial_{d})} f(z) f(z) = \sum\limits_{l = 1}^{d} \partial_{l} f(z) f_{l} (z) $
\item $ f_{(2)}(z) = \partial_{z} f_{(1)}(z) f(z) = \sum\limits_{m = 1}^{d} \partial_{m} f_{(1)} (z) f_{m}(z) = \sum\limits_{l, m = 1}^{d} \partial_{m} f_{l}(z) \partial_{l} f(z) + \sum\limits_{l, m = 1}^{d} \partial_{l m} f(z) f_{l} (z) f_{m} (z) $
\end{itemize}

Chiedendo \refsec{12.2} (2) per ogni $ f $ e diversi $ K $ si arriva a

\begin{itemize}
\item $ k = 0: \sum\limits_{i = 1}^{s} b_{i} = 1 $
\item $ k = 1: \sum\limits_{i = 1}^{s} b_{i} c_{i} = \frac{1}{2} $
\item $ k = 2: \sum\limits_{i, j = 1}^{s} b_{i} a_{ij} c_{j} = \frac{1}{6} $ e $ \sum\limits_{i = 1}^{s} b_{i} c_{i}^{2} = \frac{1}{3} $
\end{itemize} 

Il numero delle condizioni cresce drammaticamente:

\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
	Ordine & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & \ldots & 20 \\
\hline
	Numero condizioni & 1 & 2 & 4 & 8 & 17 & 37 & 85 & 200 & 486 & 1205 & \ldots & $ 20 \cdot 247 \cdot 374 $ \\
\end{tabular}
\end{center}

\subsection{Barriere di Butcher per metodi RKE}
Ci\`o che si \`e visto prima era per metodi arbitrari (espliciti o impliciti). Butcher ha determinato il numero di stadi minimo in relazione all'ordine, per ordini non minori di 9:

\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
	$ p $ & 1 & 2 & 3 & 4 & 5 & 6 & 8 & $ \ge 9 $ \\
\hline
	$ s $ & 1 & 2 & 3 & 4 & 6 & 7 & 11 & $ \ge p + 3 $ \\
\end{tabular}
\end{center}

\section{Consistenza ed estrapolazione}
\subsection{Estrapolazione di Richardson}

Sia $ \Phi $ l'evoluzione discreta di un metodo a un passo di ordine di consistenza $ r \ge 1 $, allora, se $ f \in \mathcal{C}^{r + 1} (\Omega, \mathbb{R}^{d}) $ il metodo con evoluzione

\[ \Phi_{1}(t + \tau, t) z = \frac{2^{r} \overbrace{\Phi_{0} \left( t + \tau, t + \frac{\tau}{2} \right) \Phi_{0} \left( t + \frac{\tau}{2}, t \right) z}^{= U_{1}} - \overbrace{\Phi_{0}(t + \tau, t)z}^{\hat{U}_{1}} }{2^{r} - 1} \]

ha ordine di consistenza $ r + 1 $, triplicando per\`o il costo.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-3, 0) -- (3, 0) node[right] {$ t $};
	\filldraw (-2, 0) circle (1pt) node[below] {$ t_{n} $};
	\filldraw (0, 0) circle (1pt) node[below] {$ t_{n} + \frac{\tau}{2} $};
	\filldraw (2, 0) circle (1pt) node[below] {$ t_{n + 1} $};
	\path[->] (-2, 0.1) edge[bend left] (0, 0.1);
	\path[->] (0, 0.1) edge[bend left] (2, 0.1);
	\path[->] (-2, 0.2) edge[bend left] (2, 0.2);
\end{tikzpicture}

\caption{Esempio di estrapolazione di Richardson}
\end{center}
\end{figure}

\begin{proof}
Si fissi $ (t, z) \in \Omega $ e $ \tau > 0 $ sufficientemente piccolo e si ponga:

\begin{itemize}
\item $ u_{\frac{1}{2}} = \varphi \left( t + \frac{\tau}{2}, t \right) z $, che \`e la soluzione esatta dopo mezzo passo;
\item $ u_{1} = \varphi \left( t + \tau, t \right) z $, che \`e la soluzione esatta dopo un passo;
\item $ \hat{U}_{1} = \Phi_{0} (t + \tau, t) z $;
\item $ U_{\frac{1}{2}} = \Phi_{0} \left( t + \frac{\tau}{2}, t \right) z $;
\item $ U_{1} = \Phi_{0} \left( t + \tau, t + \frac{\tau}{2} \right) z $.
\end{itemize}

Poich\'e $ f \in \mathcal{C}^{r + 1} (\Omega, \mathbb{R}^{d}) $, uno sviluppo di Taylor fornisce

\begin{equation}	\label{eq:Vrichardson}
u_{1} - \hat{U}_{1} = \underbrace{C(t, z) \tau^{r + 1}}_{\varepsilon(t, z, \tau)} + O(\tau^{r + 1})
\end{equation}

Inoltre

\[
{ u_{1} - U_{1} } = { \left[ \varphi \left( t + \tau, t + \frac{\tau}{2} \right) u_{\frac{1}{2}} - \varphi \left( t + \tau, t + \frac{\tau}{2} \right) \right] } + \\ { \left[ \varphi \left( t + \tau, t + \frac{\tau}{2} \right) - \Phi_{0} \left( t + \tau, t + \frac{\tau}{2} \right) U_{\frac{1}{2}} \right] } { \eqdef I_{1} + I_{2} }
\]

Ora:

\begin{itemize}
\item $ I_{1} = (1 + O(\tau)) \underbrace{(u_{\frac{1}{2}} - U_{\frac{1}{2}})}_{\text{consistenza su mezzo passo: } C(t, z) \left( \frac{\tau}{2} \right)^{r + 1} + O(\tau^{r + 2})} = 2^{-(r + 1)} C(t, z) \tau^{r + 1} + O(\tau^{r + 2}) $
\item $ I_{2} = \underbrace{C \left(t + \frac{\tau}{2}, U_{\frac{1}{2}} \right)}_{= C(t, z) + O(\tau)} \left( \frac{\tau}{2} \right)^{r + 1} + O(\tau^{r + 2}) = 2^{-(r + 1)} C(t, z) \tau^{r + 1} + O(\tau^{r + 2}) $
\end{itemize}

Di conseguenza

\begin{equation}	\label{eq:W2richardson}
u_{1} - U_{1} = I_{1} + I_{2} = 2^{-r} C(t, z) \tau^{r + 1} + O(\tau^{r + 2})
\end{equation}

Considerando 

\[ \frac{2^{r} (\ref{eq:W2richardson}) - (\ref{eq:Vrichardson})}{2^{r} - 1} \]

si ottiene:

\[ u_{1} - \frac{2^{r} U_{1} - \hat{U}_{1}}{2^{r} - 1} = O(\tau^{r + 2}) \]
\end{proof}

\section{Passi adattivi - idea e potenziale}	\label{section:16}
\subsection{Passi da seguire}

Sia $ \Phi $ l'evoluzione di un metodo a un passo per

\[ u' = f(\cdot, u) \]

con $ f \in \mathcal{C}^{0} (\Omega, \mathbb{R}^{d}) $ e $ (t_{0}, \underline{v}) \in \Omega $. Si ponga $ u(t) = \varphi(t, t_{0}) v $ e

\[
\begin{cases}
U_{0} \defeq v \\
U_{n + 1} \defeq \Phi(t_{n + 1}, t_{n}) U_{n}
\end{cases}
\]

dove $ t_{n + 1} = t_{n} + \tau_{n} $ e  $ t_{N} = T $ e i $ \tau_{i} $, con $ i = 0, \dotsc, N - 1 $, sono da scegliere.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-4, 0) -- (4, 0) node[right] {$ t $};
	\filldraw (-3, 0) circle (1pt) node[above] {$ t_{0} $};
	\draw (-2.5, 0) node[below] {$ \tau_{0} $};
	\filldraw (-2, 0) circle (1pt) node[above] {$ t_{1} $};
	\draw (-1.75, 0) node[below] {$ \tau_{1} $};
	\filldraw (-1.5, 0) circle (1pt) node[above] {$ t_{2} $};
	\draw (-0.25, 0) node[below] {$ \ldots $};
	\filldraw (1, 0) circle (1pt) node[above] {$ t_{i} $};
	\draw (1.75, 0) node[below] {$ \ldots $};
	\filldraw (2, 0) circle (1pt) node[above] {$ t_{N-1} $};
	\draw (2.5, 0) node[below] {$ \tau_{N-1} $};
	\filldraw (3, 0) circle (1pt) node[above] {$ t_{N} = T $};
\end{tikzpicture}

\caption{Esempio di griglia adattiva}
\end{center}
\end{figure}

Ci interessa una scelta "ottima", cio\`e del tipo: fissato $ N $, si minimizzi $ {\vert u(T) - U_{N} \vert} $, $ {\max\limits_{n = 1, \dotsc, N} \vert u(t_{n}) - U_{n} \vert } $ oppure $ { \sup\limits_{t \in [t_{0}, T]} \vert u(t) - U_{t} \vert } $, con $ U(t) $ da definire. In alternativa si pu\`o considerare fissato $ \varepsilon > 0 $ la minimizzazione di $ N $ con $ { \vert u(T) - U_{N} \vert \le \varepsilon } $.

\subsection{Un problema di approssimazione del modello}
Sia $ u \in \mathcal{C}^{0} ([t_{0}, T]) $ e si consideri

\[ \inf\limits_{t_{1} < \dotsb < t_{N}} \Vert u - \overline{u} \Vert_{[t_{0}, T]} \]

con $ \Vert u - s \Vert_{[t_{0}, T]} \defeq \sup\limits_{t \in [t_{0}, T]} \Vert u(t) - s(t) \Vert $ e $ \overline{u}_{\big\vert [t_{n}, t_{n + 1})} = u(t_{n}) $

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-1, 0) -- (5, 0) node[right] {$ t $};
	\draw[->] (0, -1) -- (0, 5) node[above] {$ u, \overline{u} $};
	\draw[dotted] (0, 0.4) -- (0, 0) node[below left] {$ t_{0} $};
	\draw[dotted] (0.5, 0.5) -- (0.5, 0) node[below] {$ t_{1} $};
	\draw[dotted] (1.5, 1) -- (1.5, 0) node[below] {$ t_{2} $};
	\draw[dotted] (1.9, 2) -- (1.9, 0) node[below] {$ t_{3} $};
	\draw[dotted] (2.7, 3) -- (2.7, 0) node[below] {$ t_{4} $};
	\draw[dotted] (3.2, 3.2) -- (3.2, 0) node[below] {$ t_{5} $};
	\draw[dotted] (4, 4) -- (4, 0) node[below] {$ t_{6} $};
	
	\draw[black] plot [smooth] coordinates {(0, 0.4) (0.5, 0.5) (1, 0.9) (1.5, 1) (1.9, 2) (2.3, 2.2) (2.7, 3) (3.2, 3.2) (4, 4) (4.5, 4.15)} node[above right] {$ u $};
	
	\draw (0, 0.4) -- (0.5, 0.4);
	\draw (0.5, 0.5) -- (1.5, 0.5);
	\draw (1.5, 1) -- (1.9, 1);
	\draw (1.9, 2) -- (2.7, 2);
	\draw (2.7, 3) -- (3.2, 3);
	\draw (3.2, 3.2) -- (4, 3.2);
	\draw (4, 4) -- (4.5, 4) node[below right] {$ \overline{u} $};

\end{tikzpicture}

\caption{Esempio di grafico di $ u $ (funzione continua) e di $ \overline{u} $ (funzione a tratti)}
\end{center}
\end{figure}

Se $ u \in \mathcal{C}^{1} ([t_{0}, T]) $ allora, con $ t \in [t_{0}, T] $

\[ \vert u(t) - u(t_{n}) \le \left\vert \int\limits_{t_{n}}^{t} u' \right\vert \le \int\limits_{t_{n}}^{t} \vert u' \vert \]

e quindi 

\[ \sup\limits_{t \in [t_{n}, t_{n + 1}]} \vert u(t) - u(t_{n}) \vert \le \int\limits_{t_{n}}^{t_{n + 1}} \vert u' \vert \]


Se $ \tau_{n} = \tau \defeq \frac{T - t_{0}}{N} $, ne segue che

\[ \Vert u - \overline{u} \Vert_{[t_{0}, T]} \le \frac{T - t_{0}}{N} \sup\limits_{t \in [t_{0}, T]} \vert u' \vert \]

ma se $ \tau_{n} $ tale che

\[ \int\limits_{t_{n}}^{t_{n + 1}} \vert u' \vert = \frac{1}{N} \int\limits_{t_{0}}^{T} \vert u' \vert \]

si ha che

\[ \Vert u - \overline{u} \Vert_{[t_{0}, T]} \le \frac{1}{N} \int\limits_{t_{0}}^{T} \vert u' \vert \]

Si osserva che, se $ u_{p} (t) = t^{p} $, con $ p > 0 $ e $ t \in (0, 1) $ si ha

\[ \sup\limits_{t \in (0, 1)} \vert u_{p}' (t) \vert = \begin{cases} \infty & p < 1 \\ p & p \ge 1 \end{cases} \]

e

\[ \int\limits_{0}^{1} \vert u_{p}' \vert = \int\limits_{0}^{1} p t^{p - 1} \, \mathrm{d}t = 1 \]

\section{Passi adattivi - generazione e trasporto dell'errore}
\subsection{Generazione e trasporto dell'errore e scelta dei passi}

\refsec{16} suggerisce di equidistribuire gli "errori locali". Gli "errori locali" vengono interpretati per un metodo a un passo dato da $ \Phi $. Scrivendo

\[ u(t) = \varphi(t, t_{0}) v \]

e

\[
\begin{cases}
U_{0} = v \\
U_{n + 1} = \Phi(t_{n + 1}, t_{n}) U_{n}
\end{cases}
\]

si ha

\[ \varphi(t_{1}, t_{0}) v - \Phi(t_{1}, t_{0}) v = \varepsilon(t_{0}, v; \tau_{0}) \]

dove:
\begin{itemize}
\item $ \varphi(t_{1}, t_{0}) v $ non \`e calcolabile;
\item $ \Phi(t_{1}, t_{0}) v $ \`e calcolabile;
\item $ t_{0} $ e $ v $ sono noti;
\item $ \tau_{0} $ \`e da determinare.
\end{itemize}

per il primo passo, mentre per il passo generico si ha

\begin{dmath*}
u(t_{n + 1}) - U_{n + 1} = \varphi(t_{n + 1}, t_{n}) u(t_{n}) - \Phi (t_{n + 1}, t_{n}) U_{n} = [ \underbrace{\varphi(t_{n + 1}, t_{n}) u(t_{n}) - \Phi (t_{n + 1}, t_{n}) U(t_{n})}_{\varepsilon(t_{n}, u(t_{n}), \tau_{n})}] + [\underbrace{\Phi (t_{n + 1}, t_{n}) U(t_{n}) - \varphi(t_{n + 1}, t_{n}) u(t_{n})}_{\text{trasporto tramite $ \Phi $. "Stabilit\`a discreta"}}]
\end{dmath*}

oppure

\begin{dmath*}
u(t_{n + 1}) - U_{n + 1} = \varphi(t_{n + 1}, t_{n}) u(t_{n}) - \Phi (t_{n + 1}, t_{n}) U_{n} = [ \underbrace{\varphi(t_{n + 1}, t_{n}) u(t_{n}) - \varphi (t_{n + 1}, t_{n}) U_{n}}_{\text{trasporto tramite $ \varphi $. "Stabilit\`a continua"}}] + [\underbrace{\varphi (t_{n + 1}, t_{n}) U_{n} - \Phi(t_{n + 1}, t_{n}) U_{n}}_{\varepsilon(t_{n}, U_{n}, \tau_{n})}]
\end{dmath*}

Ci si limiter\`a a equidistribuire (sostituzioni calcolabili) gli errori di consistenza lungo la soluzione discreta.

\subsection{Un esempio con il residuo}
Si riconsideri l'esempio

\[
\begin{cases}
u(1) = 1 \\
u' = t^{p} u
\end{cases}
\]

in $ (0, 1) $ con il metodo di Eulero esplicito. Si sostituisca l'errore di consistenza con il residuo (locale)

\[ R(t_{n}) = U(t_{n}) - U(t_{n - 1}) - \int\limits_{t_{n - 1}}^{t_{n}} f(t, U(t)) \, \mathrm{d}t = \int\limits_{t_{n - 1}}^{t_{n}} f(t_{n - 1}, U_{n - 1}) - f(t, U(t)) \, \mathrm{d}t \] 

dove $ 1 = t_{0} > t_{1} > \dotsb > t_{N - 1} > t_{N} = 0 $ e $ U $ \`e il poligono di Eulero. La relazione

\[ \mathop{toll} \approx R(t_{n}) = \int\limits_{t_{n}}^{t_{n - 1}} t_{n -1}^{p} U_{n - 1} - t^{p} U(t) \, \mathrm{d}t \approx U_{n - 1} \int\limits_{t_{n}}^{t_{n - 1}} \underbrace{t_{n - 1}^{p} - t^{p}}_{\approx p t_{n - 1}^{p} (t - t_{n})} \, \mathrm{d}t \approx U_{n - 1} p t_{n - 1}^{p - 1} \frac{1}{2} \tau_{n - 1}^{2} \]

suggerisce $ \tau_{n - 1} = \sqrt{\frac{2 \mathop{toll}}{p t_{n - 1}^{p - 1} U_{n - 1}}} $

Si osserva numericamente che:
\begin{enumerate}
\item la scelta per i passi proposta riprende l'EOC (\emph{Experimental order of convergence}, ordine di convergenza sperimentale) 1 nei casi $ p \in (-1, 0) $ al posto di $ 1 + p $;
\item per $ p \gg 1 $ l'errore \`e pi\`u piccolo rispetto a passi uniformi, con l'aumentare di $ p $ questa differenza cresce.
\end{enumerate}

\section{Passi adattivi - stimatori con dilemma}
\subsection{Stimatori per l'errore di consistenza}	\label{section:18.1}

Dati un'evoluzione $ \varphi $ e un metodo ad un passo $ \Phi $ di ordine $ p $ si vuole approssimare numericamente l'errore di consistenza

\[ \varepsilon(t, z, \tau) = \varphi(t, z, \tau) - \Phi(t, z, \tau) \]

Sia $ \Phi' $ un metodo ad un passo di ordine di consistenza $ p' > p $. Fissando $ (t, z) $, si supponga che:
\begin{itemize}
\item $ \frac{\tau^{p + 1}}{\varepsilon(t, z, \tau)} = \mathcal{O}(1) $ per $ \tau \to 0 $
\item $ \varepsilon'(t, z, \tau) = \varphi(t, z, \tau) - \Phi(t, z, \tau) = \mathcal{O}(\tau^{p' + 1}) $
\end{itemize}

Quindi
\[ \forall\ \alpha > 0, \exists\ \tau^{\ast} = \tau^{\ast}(\alpha) : \forall\ \tau \in (0, \tau^{\ast}), \vert \varepsilon'(t, z, \tau) \vert \le \alpha \vert \varepsilon(t, z, \tau) \vert \]

Ci\`o implica che 

\begin{dmath*}
{\vert \Phi'(t + \tau, t)z - \Phi(t + \tau, t)z \vert} \le {\vert \Phi'(t + \tau, t)z - \varphi(t + \tau, t)z \vert} + {\vert \varphi(t + \tau, t)z - \Phi(t + \tau, t)z \vert} = {\vert \varepsilon'(t, z, \tau) \vert} + {\vert \varepsilon(t, z, \tau) \vert} \le {(1 + \alpha) \vert \varepsilon(t, z, \tau) \vert}
\end{dmath*}

Se si sceglie $ \alpha < 1 $ si ha anche 

\begin{dmath*}
{\vert \Phi(t + \tau, t)z - \Phi'(t + \tau, t)z \vert} \ge {\vert \Phi(t + \tau, t)z - \varphi(t + \tau, t)z \vert} - {\vert \underbrace{\Phi'(t + \tau, t)z - \varphi(t + \tau, t)z \vert}_{= \varepsilon'(t, z, \tau)}} \ge {(1 + \alpha) \vert \varepsilon(t, z, \tau) \vert}
\end{dmath*}

Di conseguenza

\begin{equation}	\label{eq:stima181}
\frac{1}{1 + \alpha} \vert [\varepsilon](t, z, \tau) \vert \le \vert \varepsilon(t, z, \tau) \vert \le \frac{1}{1 - \alpha} \vert [\varepsilon(t, z, \tau)](t, z, \tau) \vert
\end{equation}

dove $ [\varepsilon](t, z, \tau) \defeq \Phi'(t, z, \tau) - \Phi(t, z, \tau) $.

\subsection{Un dilemma}
\refsec{18.1} suggerisce che $ [\varepsilon] $ pu\`o essere usato come sostituzione per $ \varepsilon $ per $ \tau $ sufficientemente piccolo. % e...
Comunque in questo caso $ \Phi'(t + \tau, t)z $ \`e disponibile e approssima $ \varphi(t + \tau, t)z $ meglio di $ \Phi(t + \tau, t)z $ ma per $ \varepsilon'(t, z, \tau) $ non si dispone di uno stimatore con $ (\ref{eq:stima181}) $ in \refsec{18.1}. In particolare $ [\varepsilon] $ sovrastimer\`a $ \varepsilon' $ per $ \tau $ piccolo.

Cosa \`e da preferire tra approssimazioni accurati o stimatori accurati? L'esperienza suggerisce la prima.

\subsection{Metodi di Runge-Kutta immersi}
Questo \`e lo schema di Butcher per  i Runge-Kutta immersi

\begin{center}
\begin{tabular}{c|ccc}
                  &   &       & \\
$ \underline{c} $ &   & $ A $ & \\
				  &   &       & \\
\hline
				  &   & $ \underline{b}^{T} $ & \\
\hline
				  &   & $ \hat{\underline{b}}^{T} $ & \\
\end{tabular}
\end{center}

Il metodo dato da $ \underline{b}^{T} $ ha ordine $ p $ mentre quello dato da $ \hat{\underline{b}}^{T} $ ha ordine $ \hat{p} = p - 1 $. Per $ U_{n + 1} = U_{n} + \tau \sum\limits_{i = 1}^{s} b_{i} k_{i} $ e si stima l'errore di consistenza con $ \tau \sum\limits_{i = 1}^{s} (b_{i} - \hat{b}_{i}) k_{i} $.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%										CONTROLLARE LE LEZIONI 19 E 20!
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Stabilit\`a}
\setcounter{section}{18}
\section{Buona posizione, buon condizionamento e stabilit\`a asintotica}
\subsection{Dipendenza in $ T < \infty $}

Sia $ \varphi $ l'evoluzione associata a $ u' = f(\cdot, u) $. Inoltre siano $ (t_{0}, v) \in \Omega $ e $ T > t_{0} $ tale che $ \varphi(T, t_{0}) v $ \`e ben definito. Si consideri

\begin{equation}	\label{eq:evol}
\varphi(T, t_{0}) v = ?
\end{equation}

sotto perturbazioni di $ v \in \mathbb{R}^{d} $. Allora: 	% TODO: appunti di Manuel contengono cose non chiare
\begin{enumerate}
\item se $ \varphi(T, t_{0}) v $ \`e continua in $ v $, cio\`e $ \forall\ \varepsilon > 0, \exists\ \delta = \delta(\varepsilon) > 0 : \forall\ w \in \mathbb{R}^{d}, \vert v - w \vert < \delta \implies \begin{cases}
\exists\ \varphi (T, t_{0}) w \\
\vert \varphi (T, t_{0}) v - \varphi(T, t_{0}) w \vert < \varepsilon
\end{cases} $ allora $ (\ref{eq:evol}) $ si dice ben posto (secondo Hadamard);
\item se esistono $ \delta^{\ast} > 0 $ "non troppo piccolo" e $ k $ "non troppo grande" tali che $ \forall\ w \in \mathbb{R}^{d} $ si ha che $ \vert w - v \vert < \delta^{\ast} \implies \begin{cases}
\exists\ \varphi(t, t_{0}) w \\
\vert \varphi(T, t_{0}) v - \varphi(T, t_{0}) w \vert < k \vert v - w \vert
\end{cases} $ allora $ (\ref{eq:evol}) $ si dice ben condizionato. Questo equivale a dire che $ \varphi(T, t_{0}) $ \`e lipschitziana con costante "non troppo grande" in un intorno "non troppo piccolo";
\item al posto di $ (\ref{eq:evol}) $ si pu\`o considerare anche $ \varphi(t, t_{0}) v $ per $ t \in (t_{0}, T] $ e sostituire nelle definzioni precedenti $ \vert \varphi(T, t_{0}) v - \varphi(T, t_{0}) w \vert $ con $ \max\limits_{t \in [t_{0}, T]} \vert \varphi(T, t_{0}) v - \varphi(T, t_{0}) w \vert $.
\end{enumerate}

\subsection{Dipendenza in $ [t_{0}, \infty) $}
\begin{enumerate}
\item Se per $ \varepsilon > 0, \exists\ \delta = \delta (\varepsilon) > 0 : \forall\ w \in \mathbb{R}^{d}, \vert w - v \vert < \delta \implies \forall\ t \in [t_{0}, T] \begin{cases}
\exists\ \varphi(t, t_{0}) w \\
\vert \varphi (t, t_{0}) v - \varphi(t, t_{0}) w \vert < \varepsilon
\end{cases} $ allora $ \varphi(t, t_{0}) v $ si dice stabile. Se $ f(v) = 0 $ ovvero $ \varphi(t, t_{0}) v = v $ (\`e stabile) questo equivale alla stabilit\`a secondo Lyapunov;
\item Se $ \varphi(\cdot, t_{0}) v $ \`e stabile e $ \exists\ \delta > 0 : \forall\ w \in \mathbb{R}^{d}, \vert w - v \vert < \delta \implies \lim\limits_{t \to \infty} \vert \varphi(t, t_{0}) v - \varphi(t, t_{0}) w \vert = 0 $ allora $ \varphi(\cdot, t_{0}) v $ si dice asintoticamente stabile. Tale propriet\`a non \`e facile riprodurlo nel discreto;
\item Se non vale il punto $ 1. $ allora $ \varphi(\cdot, t_{0}) v $ si dice instabile. 
\end{enumerate}

\section{Funzione e dominio di stabilit\`a}
\subsection{Problemi test di Dahlquist}	\label{section:20.1}
Il problema: trovare $ u \in \mathcal{C}^{1}(\mathbb{R}, \mathbb{C}) $ tale che 

\[
\begin{cases}
u' = \lambda u \\
u(0) = 1
\end{cases}
\]

in $ \mathbb{R} $ si dice problema-test di Dahlquist per $ \lambda \in \mathbb{C} $. Si osservi che $ u(t) = e^{\lambda t} $ con $ t \in \mathbb{R} $ e quindi:
\begin{itemize}
\item $ \lim\limits_{t \to \infty} \vert u(t) \vert = 0 $ se e solo se $ \Re (\lambda) < 0 $ (asintoticamente stabile);
\item $ \forall\ t \ge 0, \vert u(t) \vert = \vert u(0) \vert $ se e solo se $ \Re (\lambda) = 0 $;
\item $ \lim\limits_{t \to \infty} \vert u(t) \vert = \infty $ se e solo se $ \Re (\lambda) > 0 $.
\end{itemize}

Inoltre $ u^{(p + 1)} (t) = \lambda^{p + 1} e^{\lambda t} = \lambda^{p + 1} u(t) $, cio\`e le derivate modulo potenze di $ \lambda $ hanno lo stesso comportamento per $ t \to +\infty $ (se il problema \`e asintoticamente stabile le derivate vanno a $ 0 $). Le derivate vanno come la funzione.

\subsection{Funzione di stabilit\`a}	\label{section:20.2}
Siano $ A $, $ b $ e $ c $ di un metodo RK e si denoti con $ \Phi_{\lambda} (\tau) : \mathbb{C} \to \mathbb{C} $ la sua evoluzione per \refsec{20.1} usando l'autonomia dell'EDO. Dato $ z \in \mathbb{C} $ si scriva $ z = \lambda \tau $ con $ \lambda \in \mathbb{C} $ e $ \tau \ge 0 $, si definisca la funzione di stabilit\`a del metodo considerato tramite $ R(z) = \Phi_{\lambda} (\tau) $. Si ha che:
\begin{enumerate}
\item $ R $ \`e ben definita;
\item la soluzione del metodo considerato per \refsec{20.1} con passo uniforme $ \tau > 0 $ \`e $ U_{n} = R(\tau \lambda)^{n} $ con $ n \in \mathbb{N} $.
\end{enumerate}

\begin{proof}
\noindent
\begin{enumerate}
\item Siano $ \tau, \tilde{\tau} \ge 0 $ e $ \lambda, \tilde{\lambda} \in \mathbb{C} $ tali che $ \tilde{\tau} \tilde{\lambda} = \tau \lambda $ con $ \tau > 0 $. Gli stadi $ k_{i} $ e $ \tilde{k}_{i} $ sono rispettivamente dati da $ k_{i} = \lambda \left( 1 + \tau \sum\limits_{j = 1}^{s} a_{ij} k_{j} \right) $ e $ \tilde{k}_{i} = \tilde{\lambda} \left( 1 + \tilde{\tau} \sum\limits_{j = 1}^{s} a_{ij} \tilde{k}_{j} \right) $  da cui, moltiplicando rispettivamente per $ \tau $ e $ \tilde{\tau} $ si ricava $ \tau k_{i} = \tau \lambda \left( 1 + \tau \sum\limits_{j = 1}^{s} a_{ij} k_{j} \right) $ e $ \tilde{\tau}\tilde{k}_{i} = \tilde{\tau} \tilde{\lambda} \left( 1 + \tilde{\tau} \sum\limits_{j = 1}^{s} a_{ij} \tilde{k}_{j} \right) $, per $ i = 1, \dotsc, s $. Da ci\`o si ottiene $ \tau k_{i} = \tilde{\tau} \tilde{k}_{i} $, di conseguenza $ 1 + \sum\limits_{j = 1}^{s} b_{i} (\tau k_{i}) = 1 + \sum\limits_{j = 1}^{s} b_{i} (\tilde{\tau} \tilde{k}_{i}) $, ovvero $ \Phi_{\lambda}(\tau) = \Phi_{\tilde{\lambda}} (\tilde{\tau}) $;
\item $ U_{n + 1} = \Phi_{\lambda} (\tau) U_{n} $ per linearit\`a \`e uguale a $ \Phi_{\lambda} (\tau) U_{n} = R(\lambda \tau) U_{n} $ e per induzione si ottiene che $ U_{n + 1} = R(\lambda \tau)^{n} $.
\end{enumerate}
\end{proof}

\subsection{Dominio di stabilit\`a}
Se $ U_{n} $ \`e soluzione discreta di \refsec{20.1} allora \refsec{20.2} (2.) implica:
\begin{itemize}
\item $ \lim\limits_{n \to \infty} U_{n} = 0 \iff \vert R(\lambda \tau) \vert < 1 $;
\item $ \forall\ n \in \mathbb{N}, \vert U_{n} \vert = \vert U_{0} \vert $;
\item $ \vert U_{n} \vert \to \infty \iff \vert R(\tau \lambda) \vert > 1 $.
\end{itemize}

Si introduce il dominio di stabilit\`a del metodo consistente:

\[ S = \{ z \in \mathbb{C} : \vert R(z) \vert < 1 \} \]

\section{Stabilit\`a di EDO autonomi lineari omogenei}
\subsection{EDO autonomi lineari omogenei e stabilit\`a}	\label{section:21.1}

Si consideri l'EDO

\begin{equation}	\label{equation:EDOmultidim}
u' = A u
\end{equation}

dove $ A \in \Mat_{d}(\mathbb{C}) $ e $ d \in \mathbb{N} $. I seguenti enunciati sono equivalenti:

\begin{enumerate}
\item Una soluzione di $ (\ref{equation:EDOmultidim}) $ \`e stabile o asintoticamente stabile; % ovvero exists epsilon :  | u - upert | < eps
\item Ogni soluzione di $ (\ref{equation:EDOmultidim}) $ \`e stabile o asintoticamente stabile;
\item La soluzione $ u \equiv 0 $ di $ (\ref{equation:EDOmultidim}) $ \`e stabile o asintoticamente stabile; % basta perturbare quella per capire cosa succede
\item $ \forall\ u $ soluzione di $ (\ref{equation:EDOmultidim}) $ vale, $ \forall\ t \ge 0, \vert u(t) \vert \le C \vert u(0) \vert $, con $ C $ indipendente da $ u $ ($\lim\limits_{t \to \infty} u(t) = 0 $).
\end{enumerate}

Se uno e quindi tutti soddisfano $ (\ref{equation:EDOmultidim}) $ si dice stabile o asintoticamente stabile. Data $ M \in \GL_{d}(\mathbb{C}) $, questo equivale anche a 

\begin{enumerate}
\item[5.] $ u' = M A M^{-1} u $ \`e stabile o asintoticamente stabile.
\end{enumerate}

\begin{proof}
Per esercizio.
\end{proof}

\subsection{Spettro}
Dato $ A \in \Mat_{d}(\mathbb{C}) $ sia 
\[ \sigma(A) = \{ \lambda \in \mathbb{C} : \det(A - \lambda I) = 0 \} \]

lo spettro di $ A $, dove $ I $ \`e la matrice identit\`a. Per ogni autovalore $ \lambda \in \sigma(A) $, sia $ i(\lambda) $ la dimensione massima del blocco di Jordan associato all'autovalore $ \lambda $. Infine si pone:

\[ \nu(A) \defeq \max\limits_{\lambda \in \sigma(A)} \Re(\lambda) \]

detto l'ascissa spettrale di $ \lambda $. 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\draw[->] (-4,0) -- (4,0) node[right] {$ \mathbb{R} $};
	\draw[->] (0,-2) -- (0,4) node[above] {$ \mathbb{R} $};

	\draw (-3, 2) circle (1pt);
	\draw (2, 3) circle (1pt);
	\draw (-1,1.5) circle (1pt);
	\draw (0.2,-1.5) circle (1pt);
	\draw (3,-2) circle (1pt);
	\draw (-2,-1) circle (1pt);

	\draw[dashed] (2,3) -- (2,0) node[below] {$ \nu(\lambda) $};
\end{tikzpicture}

\caption{Esempio di discretizzazione dell'asse delle $ t $. $ \tau $ \`e la distanza tra i vari nodi ed \`e costante.}
\end{center}
\end{figure}

\subsection{Caratterizzazione della stabilit\`a}	\label{section:21.3}
Sia $ A \in \Mat_{d}(\mathbb{C}) $. L'EDO $ u' = A u $ \`e stabile (o asintoticamente stabile) se e solo se

\[ \nu(A) \le 0 \text{ (o } \nu(A) < 0) \]

e

\[ \forall\ \lambda \in \sigma(A), \Re(\lambda) = 0 \implies i(\lambda) = 1 \]

\begin{proof}
In virt\`u di \refsec{21.1} basta verificare l'equivalenza per la forma normale di Jordan. Inoltre grazie all'esercitazione, basta considerare un blocco di Jordan

\[ J = \lambda I + N \in \Mat_{k} (\mathbb{C}) \]

dove $ k \le i(\lambda) $, $ I = \begin{pmatrix} 
1 & 0 & 0 & \ldots & 0 \\
0 & 1 & 0 & \ldots & 0 \\
0 & 0 & \ddots & \ddots & 0 \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & 1 \\
\end{pmatrix} $, mentre $ N = \begin{pmatrix}
0 & 1 & 0 & \ldots & 0 \\
0 & 0 & 1 & \ldots & 0 \\
0 & 0 & \ddots & \ddots & 0 \\
\vdots & \vdots & \ddots & \ddots & 1 \\
0 & 0 & 0 & \ldots & 0 \\\end{pmatrix} $. Si osserva che

\begin{equation}	\label{equation:exptJ}
\exp(t J) = \exp(t(\lambda I + N)) = \exp(t \lambda I) \exp(t N) \stackrel{N^{k} = 0}{=} e^{t \lambda} \left( \sum\limits_{l = 0}^{k - 1} \frac{t^{l}}{l!} N^{l} \right)
\end{equation}

Denotando la norma matriciale indotta dalla norma euclidea $ \vert \cdot \vert $ in $ \mathbb{C}^{d} $ con $ \Vert \cdot \Vert $ si ha

\[ \Vert \exp (tJ) \Vert \le e^{t \Re(\lambda)} \left( \sum\limits_{l = 0}^{k - 1} \frac{t^{l}}{l!} \Vert N \Vert^{l} \right) \]

da cui si evince la sufficienza dei criteri su $ \sigma(A) $. Per la necessit\`a basta applicare $ (\ref{equation:exptJ}) $ su $ (0, 0, \dotsc, 0, 1) \in \mathbb{C}^{k} $ e $ \left\vert \exp (t J) \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \end{pmatrix} \right\vert = \left\vert \left( \frac{t^{k - 1}}{(k - 1)!}, \dotsc, t, 1 \right) \right\vert $ rimane limitato.
\end{proof}

\section{Stabilit\`a e metodi di Runge-Kutta espliciti} % 22
\subsection{Funzioni di stabilit\`a dei metodi RK}	\label{section:22.1}
Sia $ R $ la funzione di stabilit\`a del metodo di Runge-Kutta 

\begin{center}
\begin{tabular}{c|c}
$ \underline{c} $ & $ A $ \\
\hline
				  & $ \underline{b}^{T} $ \\
\end{tabular}
\end{center}

Allora 

\begin{enumerate}
\item $ R(z) = 1 + z b^{T} (I - zA)^{-1} \underline{e} $ con $ \underline{e} = (1 , \dotsc, 1)^{T} \in \mathbb{R}^{s} $;
\item Esistono polinomi $ P, Q \in \mathbb{P}_{s} $ senza divisori comuni e $ P(0) = Q(0) = 1 $ tali che

\[ R(z) = \frac{P(z)}{Q(z)} \]

con $ z \in \mathbb{C} $ e $ Q(z) \ne 0 $;
\item Se il metodo \`e esplicito allora $ R \in \mathbb{P}_{s} $
\end{enumerate}

\begin{proof}
\noindent
\begin{enumerate}
\item Esercizio E8.1;	% TODO: aggiungere
\item Applicando la regola di Cramer si ottiene:

\[ [(I - zA)^{-1} \underline{e} ]_{i} = \frac{P_{i} (z)}{\det (I - zA)} \]

con $ P_{i} \in \mathbb{P}_{s - 1} $ per $ i = 1, \dotsc, s $. Inoltre, definendo

\[ Q(z) = \det (I - zA) \]

e

\[ P(z) = 1 + z \sum\limits_{i = 1}^{s} b_{i} \underbrace{P_{i}(z)}_{\in \mathbb{P}_{s - 1}} \]

si ha

\begin{itemize}
\item $ Q \in \mathbb{P}_{s} $ e $ Q(0) = \det (I) = 1 $;
\item $ P \in \mathbb{P}_{s} $ e $ P(0) = 1 + 0 = 1 $;
\item $ R(z) = 1 + zb^{T} (I - zA)^{-1} \underline{e} = \frac{P(z)}{Q(z)} $
\end{itemize}

\item Se $ a_{ij} = 0 $ per $ i \ge j $, allora $ I - zA $ \`e triangolare e quindi $ Q(z) = \det (I - zA) = 1 $.
\end{enumerate}
\end{proof}

\subsection{Dominio di stabilit\`a di metodi RKE}	\label{section:22.2}
Per ogni metodo Runge-Kutta esplicito (RKE) consistente esiste un $ r \in \mathbb{R}^{+} $ tale che il suo dominio di stabilit\`a soddisfa

\[ S = \{ z \in \mathbb{C} : \vert R(z) \vert \le 1 \} \subseteq B_{r}(0) \defeq \{ z \in \mathbb{C} : \vert z \vert < r \} \]

\begin{proof}
Combinando \refsec{22.1} (c) e E7.3 (c) si ottiene:

\[ R \in \mathbb{P}_{s} \setminus \mathbb{P}_{0} \]

cio\`e

\[ R(z) = \sum\limits_{i = 0}^{k} \alpha_{i} z^{i} \]

con $ \alpha_{k} \ne 0 $ e $ k \in \{ 1, \dotsc, s \} $. Di conseguenza,

\[ \vert R(z) \vert \ge \vert \alpha_{k} \vert \vert z \vert^{k} - \sum\limits_{i = 0}^{k} \vert \alpha_{i} \vert \vert z \vert^{i} = \underbrace{\vert \alpha_{k} \vert}_{\ne 0} \underbrace{\vert z \vert^{k}}_{\to \infty} \left( 1 - \underbrace{\sum\limits_{i = 0}^{k - 1} \frac{\vert \alpha_{i} \vert}{\vert \alpha_{k} \vert} \vert z \vert^{i - k}}_{\to 0 \text{ per } \vert z \vert \to \infty} \right) \]

Dunque esiste $ r > 0 $ tale che $ \vert R(z) \vert \ge 1 $ per $ \vert z \vert \ge r $. 
\end{proof}

\subsection{Stabilit\`a e accuratezza}
Si consideri l'applicazione di un metodo RKE sul problema di Dahlquist

\begin{equation}	\label{equation:Dahlquist}
\begin{cases}
u' = \lambda u \\
u(0) = 1 
\end{cases}
\end{equation}

con $ \lambda \ll 0 $. \refsec{22.2} mostra che pu\`o valere $ \tau \lambda \in S $ solo se $ \vert \tau \lambda \vert < r $, ovvero $ \tau \le \frac{r}{\vert \lambda \vert} $, che richiede passi molto piccoli. Dall'altra parte la derivata $ p $-esima della soluzione soddisfa

\[ \vert u^{(p)} \vert \le \vert \lambda \vert^{p} e^{\lambda t} \]

e quindi l'errore di consistenza permetterebbe passi pi\`u grandi andando avanti nel tempo.

\section{Stabilit\`a di ricorsioni e funzioni razionali}
\subsection{Stabilit\`a di ricorsioni lineari}

Sia $ M \in \Mat_{d}(\mathbb{C}) $ una matrice. La ricorsione $ U_{n + 1} = M U_{n} $ si dice:
\begin{enumerate}
\item stabile se $ \sup\limits_{n \in \mathbb{N}} \Vert M^{n} \Vert < +\infty $;
\item asintoticamente stabile se $ \lim\limits_{n \to \infty} \Vert M^{k} \Vert = 0 $
\end{enumerate}

Si osserva che queste propriet\`a dipendono dalla classi di equivalenza di similitudine. Pi\`u precisamente, si ha 

\begin{itemize}
\item $ 1. \iff \rho (M) \le 1 $, dove $ \rho(M) = \max\limits_{\lambda \in \sigma(M)} \vert \lambda \vert $;
\item $ 2. \iff \rho (M) < 1 $.
\end{itemize}

\begin{proof}
Basta considerare un blocco di Jordan

\[ J = \lambda I + N \in \Mat_{k}(\mathbb{C}) \]

con $ \lambda \in \sigma(M), k \le i(\lambda), I $ \`e la matrice identit\`a e $ N $ quella nilpotente definite in \refsec{21.3}. Usando le propriet\`a di $ N $, si ottiene $ n \ge k $. 

\begin{dmath*}
{ \vert \lambda \vert^{n} \left\vert \left( \begin{pmatrix} n \\ k - 1 \end{pmatrix} \lambda^{-k + 1}, \dotsc, \begin{pmatrix} n \\ 1 \end{pmatrix} \lambda^{-1}, 1 \right) \right\vert } = { \vert J^{n} e_{k} \vert } \le { \Vert J^{k} \Vert } \\ \le { \vert \lambda \vert^{n} \left[  1 + \vert \lambda \vert^{-1} \begin{pmatrix} n \\ 1 \end{pmatrix} \Vert N \Vert + \dotsb + \vert \lambda \vert^{-k + 1} \begin{pmatrix} n \\ k - 1 \end{pmatrix} \Vert N \Vert^{k - 1} \right] }
\end{dmath*}

da cui segue la tesi.
\end{proof}

\subsection{Spettro di matrici e funzioni razionali}	% TODO: ricontrollare e dividere in punti
Siano $ R $ una funzione razionale in $ \mathbb{C} $, cio\`e $ R = \frac{P}{Q} $ dove $ P, Q $ sono polinomi su $ \mathbb{C} $ e $ A \in \Mat_{d} (\mathbb{C}) $. Se

\[ \sigma(A) \cap \{ z \in \mathbb{C} : Q(z) = 0 \} = \varnothing \]

allora $ Q(A) $ \`e invertibile e si pone

\[ R(A) = Q(A)^{-1} P(A) \]

In questo caso si ha $ \sigma(R(A)) ) \{ R(\lambda) : \lambda \in \sigma(A) \} $ e $ i(\underbrace{\mu}_{\in \sigma(R(A))}) \le \max \{ i(\lambda) : \lambda \in \sigma(A), \mu = R(\lambda) \} $

\begin{proof}
\noindent
\begin{enumerate}
\item Per l'invertibilit\`a di $ Q(A) $ basta considerare un blocco di Jordan $ J = \lambda I + N $ come prima. Uno sviluppo di Taylor fornisce

\[ Q(J) = Q(\lambda) I + \sum\limits_{i = 1}^{\min \{\deg Q, k \}} \frac{Q^{(i)}(\lambda)}{i!} N^{i} = Q(\lambda) I + N_{Q} \]

con $ k $ il grado del blocco di Jordan, $ N_{Q} = \begin{pmatrix}
0 & \ast & \ast & \ldots & \ast \\
0 & 0 & \ast & \ldots & \ast \\
0 & 0 & \ddots & \ddots & \ast \\
\vdots & \vdots & \ddots & \ddots & \ast \\
0 & 0 & 0 & \ldots & 0 \\\end{pmatrix} $ e $ Q(I) $ \`e invertibile se e solo se $ Q(\lambda) \ne 0 $
\item Nuovamente basta considerare un blocco di Jordan. Come in 1. si scrive

\[ P(J) = P(\lambda) I + N_{p} \]

e si osserva che

\[ Q(J)^{-1} = \frac{1}{Q(J)} I  + N_{Q^{-1}} \]
poich\'e l'inverso di una matrice tridiagonale \`e una matrice tridiagonale. Quindi

\[ R(J) = Q(J)^{-1} P(J) = \left( \frac{1}{Q(\lambda)} I + N_{Q^{-1}} \right) \left( P(\lambda)I + N_{P}  \right) = \frac{P(\lambda)}{Q(\lambda)} I  + \underbrace{P(\lambda) N_{Q^{-1}} + \frac{1}{Q(\lambda)} N_{P} + N_{Q^{-1}} N_{P}}_{= \begin{pmatrix} 0 &   & \ast \\ & \ddots & \\0 &  & 0 \end{pmatrix}} \]

da cui segue l'affermazione.
\end{enumerate}
\end{proof}

\section{A-stabilit\`a e L-stabilit\`a}
\subsection{Un ausiliario: teorema del modulo massimo}	\label{section:24.1}
Sia $ R: D \to \mathbb{C} $ una funzione razionale senza zeri al denominatore ma non nel numeratore nell'aperto connesso $ D \subseteq \mathbb{C} $. Se $ \forall\ z \in D, \vert f(z) \vert \le \vert f(z_{0}) \vert $ per qualche $ z_{0} \in D $ allora $ f $ \`e costante.

\begin{proof}
Dal momento che $ R $ non ha poli in $ D $ allora $ R $ \`e olomorfo (derivabile in $ \mathbb{C} $) in $ D $ e cos\`i l'enunciato \`e un caso particolare dell'omonimo teorema di analisi complessa.
\end{proof}

\subsection{A-stabile}
Sia $ R $ la funzione di stabilit\`a di un metodo di Runge-Kutta consistente. Il metodo si dice A-stabile se una delle tre equivalenti condizioni \`e soddisfatta:
\begin{enumerate}
\item $ \forall\ d \in \mathbb{N}, A \in \Mat_{d}(\mathbb{C}), u' = Au $ asintoticamente stabile $ \implies U_{n + 1} = R(\tau A) U_{n} $ asintoticamente stabile, $ \forall\ \tau > 0 $;
\item $ \forall\ d \in \mathbb{N}, A \in \Mat_{d}(\mathbb{C}), u' = Au \text{ stabile} \implies U_{n + 1} = R(\tau A) U_{n} \text{ stabile}, \forall\ \tau > 0 $;
\item $ \mathbb{C}^{-} \defeq \{ z \in \mathbb{C} : \Re (z) \le 0 \} = \{ z \in \mathbb{C} : \vert e^{z} \vert \le 1 \} \subseteq S = \{ z \in \mathbb{C} : \vert R(z) \Vert \le 1 \} $
\end{enumerate}

\begin{proof}
Viene dimostrato solo $ 1. \iff 3. $. Similmente si dimostra $ 2. \iff 3. $.
\begin{itemize}
\item[$ 1. \implies 3. $] Dato $ z \in \mathbb{C}^{-} \setminus i\mathbb{R} $ si scrive $ z = \tau \lambda $ con $ \lambda \in \mathbb{C} \setminus i\mathbb{R} $ e $ \tau > 0 $. Quindi $ u' = \lambda u $ \`e asintoticamente stabile, allora per $ 1. $ si ha che $ U_{n + 1} = R(\tau \lambda) U_{n} $ \`e asintoticamente stabile, cio\`e $ \vert R(\tau \lambda) \vert < 1, \forall\ \tau > 0 $. Riassumento, $ \forall\ z \in \mathbb{C}, \Re (z) < 0 \implies \vert R(z) \vert < 1 $. Poich\'e $ R $ \`e continua su $ \mathbb{C}^{-} $ questo implica $ \forall\ z \in \mathbb{C}, \Re(z) \le 0 \implies \vert R(z) \vert \le 1 $, ovvero $ \mathbb{C}^{-} \subseteq S $;
\item[$ 3. \implies 1. $] Sia $ \mathbb{C}^{-} \subseteq \{ z \in \mathbb{C} : \vert R(z) \vert \le 1 \} $. In particolare, $ R $ non ha poli in $ \{ z \in \mathbb{C} : \Re(z) < 0 \} $ e, per la consistenza di $ R $, non \`e costante. Quindi il teorema del modulo massimo in \refsec{24.1} si ha che

\begin{equation}	\label{eq:modmax}
\forall\ z \in \mathbb{C}, \Re (z) < 0 \implies \vert R(z) \vert < 1
\end{equation}

Dato $ A \in \Mat_{d}(\mathbb{C}) $ e $ \tau > 0 $, se ne deduce che $ u' = Au $ \`e asintoticamente stabile $ \stackrel{\refsec{21.3}}{\implies} \forall\ \lambda \in \sigma(A), \Re(\lambda) < 0 \stackrel{(\ref{eq:modmax})}{\implies} \forall\ \lambda \in \sigma(A), \vert R(\tau \lambda) \vert < 1 \implies \rho (R(\tau A)) < 1 \implies U_{n + 1} = R(\tau A) U_{n} $ \`e stabile. 
\end{itemize}
\end{proof}

\subsection{L-stabile}
Sia $ R $ la funzione di stabilit\`a di un metodo RK consistente. Il metodo si dice L-stabile se \`e A-stabile e soddisfa una delle seguenti condizioni equivalenti:
\begin{enumerate}
\item $ \forall\ A \in \Mat_{d}(\mathbb{C}), u' = A u $ asintoticamente stabile $ \implies \lim\limits_{\tau \to \infty} \Vert R(\tau A) \Vert = 0 $;
\item $ \lim\limits_{\Re(z) \to -\infty} R(z) = 0 $;
\item $ \lim\limits_{\Re(z) \to \infty} R(z) = 0 $;
\end{enumerate}

\begin{proof}
Usufruire il fatto che $ R $ \`e quoziente di due polinomi, $ R = \frac{P}{Q} $ dove $ P \in \mathbb{P}_{s} $ e $ Q \in \mathbb{P}_{t} $ e quindi $ 1. $ e $ 3. $ sono equivalenti per $ s < t $.
\end{proof}
\end{document}
